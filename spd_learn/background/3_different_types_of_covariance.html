
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Different Types of Covariance and Computation Methods &#8212; SPD Learn 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="../_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=7073f910" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=5aa371e9"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'background/3_different_types_of_covariance';</script>
    <link rel="canonical" href="https://spdlearn.github.io/spd_learn/background/3_different_types_of_covariance.html" />
    <link rel="icon" href="../_static/spd_learn.png"/>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Limitations and When Not to Use SPD Approaches" href="4_limitations.html" />
    <link rel="prev" title="Inherent Properties and Their Exploitation" href="2_inherent_properties.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="0.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/spd_learn.png" class="logo__image only-light" alt="SPD Learn Logo"/>
    <img src="../_static/spd_learn.png" class="logo__image only-dark pst-js-only" alt="SPD Learn Logo"/>
  
  
    <p class="title logo__title"><strong>SPD</strong> Learn</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../theory.html">
    Theory
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../generated/auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="../contributing.html">
    Contributing
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/spd-learn/spd_learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/spd_learn/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="../installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../theory.html">
    Theory
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../generated/auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing.html">
    Contributing
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/spd-learn/spd_learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/spd_learn/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active has-children"><a class="reference internal" href="index.html">Background</a><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="1_spd_motivation.html">SPD Geometry: Motivation and Core Properties</a></li>
<li class="toctree-l2"><a class="reference internal" href="2_inherent_properties.html">Inherent Properties and Their Exploitation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Different Types of Covariance and Computation Methods</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_limitations.html">Limitations and When Not to Use SPD Approaches</a></li>
<li class="toctree-l2"><a class="reference internal" href="5_computation_methods.html">Computational Methods for SPD Manifolds</a></li>
<li class="toctree-l2"><a class="reference internal" href="6_deep_learning_integration.html">Deep Learning Integration: Autograd and Trivialization</a></li>
<li class="toctree-l2"><a class="reference internal" href="7_spd_layers.html">SPD Neural Network Layers: A Complete Guide</a></li>
<li class="toctree-l2"><a class="reference internal" href="8_historical_context.html">Historical Context: SPD Methods in Neuroscience</a></li>
</ul>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../geometric_concepts.html">Geometric Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="../numerical_stability.html">Numerical Stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="../glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="../references.html">References</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../theory.html" class="nav-link">Theory</a></li>
    
    
    <li class="breadcrumb-item"><a href="index.html" class="nav-link">Background</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Different Types of Covariance and Computation Methods</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="different-types-of-covariance-and-computation-methods">
<span id="different-types-of-covariance"></span><h1>Different Types of Covariance and Computation Methods<a class="headerlink" href="#different-types-of-covariance-and-computation-methods" title="Link to this heading">#</a></h1>
<p>Covariance matrices are fundamental to many neuroimaging and signal processing applications.
This section explores the different types of covariance matrices, how to compute them, and
important considerations for ensuring they remain symmetric positive definite (SPD).</p>
<p>Understanding these computational aspects is crucial for applying SPD learning methods effectively
in practice.</p>
<section id="symmetric-positive-definite-matrices-formal-definition">
<h2>Symmetric Positive Definite Matrices: Formal Definition<a class="headerlink" href="#symmetric-positive-definite-matrices-formal-definition" title="Link to this heading">#</a></h2>
<p>A matrix <span class="math notranslate nohighlight">\(\mathbf{C} \in \mathbb{R}^{n \times n}\)</span> is <strong>symmetric positive definite</strong> (SPD)
if it satisfies two conditions:</p>
<p><strong>Symmetry</strong>:</p>
<div class="math notranslate nohighlight" id="equation-symmetry-condition">
<span class="eqno">(1)<a class="headerlink" href="#equation-symmetry-condition" title="Link to this equation">#</a></span>\[\mathbf{C} = \mathbf{C}^\top\]</div>
<p><strong>Positive Definiteness</strong>:</p>
<div class="math notranslate nohighlight" id="equation-positive-definite-condition">
<span class="eqno">(2)<a class="headerlink" href="#equation-positive-definite-condition" title="Link to this equation">#</a></span>\[\mathbf{x}^\top \mathbf{C} \mathbf{x} &gt; 0, \quad \forall \mathbf{x} \in \mathbb{R}^n \setminus \{\mathbf{0}\}\]</div>
<p>Equivalently, <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> is SPD if and only if all its eigenvalues are strictly positive:</p>
<div class="math notranslate nohighlight" id="equation-eigenvalue-condition">
<span class="eqno">(3)<a class="headerlink" href="#equation-eigenvalue-condition" title="Link to this equation">#</a></span>\[\lambda_i(\mathbf{C}) &gt; 0, \quad \forall i \in \{1, \ldots, n\}\]</div>
<section id="mathematical-notation">
<h3>Mathematical Notation<a class="headerlink" href="#mathematical-notation" title="Link to this heading">#</a></h3>
<p>The space of SPD matrices is denoted as:</p>
<div class="math notranslate nohighlight" id="equation-spd-space">
<span class="eqno">(4)<a class="headerlink" href="#equation-spd-space" title="Link to this equation">#</a></span>\[\mathcal{S}_{++}^n = \{\mathbf{C} \in \mathbb{R}^{n \times n} \mid \mathbf{C} = \mathbf{C}^\top, \, \lambda_i(\mathbf{C}) &gt; 0 \, \forall i\}\]</div>
<p><strong>Important distinction</strong>: When matrices are only positive <strong>semi</strong>-definite (PSD), they may have
zero eigenvalues and belong to <span class="math notranslate nohighlight">\(\mathcal{S}_{+}^n\)</span>. Such matrices lie on the boundary of
the SPD manifold and require special handling.</p>
</section>
</section>
<section id="example-checking-spd-property">
<h2>Example: Checking SPD Property<a class="headerlink" href="#example-checking-spd-property" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">is_spd</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">tol</span><span class="o">=</span><span class="mf">1e-8</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Check if matrix C is symmetric positive definite.&quot;&quot;&quot;</span>
    <span class="c1"># Check symmetry</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">torch</span><span class="o">.</span><span class="n">allclose</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">atol</span><span class="o">=</span><span class="n">tol</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="s2">&quot;Matrix is not symmetric&quot;</span>

    <span class="c1"># Check positive definiteness via eigenvalues</span>
    <span class="n">eigvals</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">eigvals</span> <span class="o">&lt;=</span> <span class="n">tol</span><span class="p">):</span>
        <span class="k">return</span> <span class="kc">False</span><span class="p">,</span> <span class="sa">f</span><span class="s2">&quot;Matrix has non-positive eigenvalues: min=</span><span class="si">{</span><span class="n">eigvals</span><span class="o">.</span><span class="n">min</span><span class="p">()</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span>

    <span class="k">return</span> <span class="kc">True</span><span class="p">,</span> <span class="s2">&quot;Matrix is SPD&quot;</span>


<span class="c1"># Example 1: Valid SPD matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">is_spd</span><span class="p">(</span><span class="n">A</span><span class="p">))</span>  <span class="c1"># (True, &#39;Matrix is SPD&#39;)</span>

<span class="c1"># Example 2: Symmetric but not positive definite</span>
<span class="n">B</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>  <span class="c1"># Eigenvalues: [3, -1]</span>
<span class="nb">print</span><span class="p">(</span><span class="n">is_spd</span><span class="p">(</span><span class="n">B</span><span class="p">))</span>  <span class="c1"># (False, &#39;Matrix has non-positive eigenvalues...&#39;)</span>

<span class="c1"># Example 3: Not symmetric</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">],</span> <span class="p">[</span><span class="mf">3.0</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="nb">print</span><span class="p">(</span><span class="n">is_spd</span><span class="p">(</span><span class="n">C</span><span class="p">))</span>  <span class="c1"># (False, &#39;Matrix is not symmetric&#39;)</span>
</pre></div>
</div>
</section>
<section id="types-of-covariance-matrices">
<h2>Types of Covariance Matrices<a class="headerlink" href="#types-of-covariance-matrices" title="Link to this heading">#</a></h2>
<p>Different neuroimaging modalities and analysis goals require different types of covariance
matrices. Understanding these types is essential for choosing appropriate computation methods.</p>
<section id="spatial-covariance-matrix">
<h3>Spatial Covariance Matrix<a class="headerlink" href="#spatial-covariance-matrix" title="Link to this heading">#</a></h3>
<p>The <strong>spatial covariance matrix</strong> captures correlations across spatial channels (sensors,
electrodes, voxels) at a given time or within a time window.</p>
<p>For EEG/MEG data <span class="math notranslate nohighlight">\(\mathbf{X} \in \mathbb{R}^{n_C \times n_T}\)</span> (channels x timepoints):</p>
<div class="math notranslate nohighlight" id="equation-spatial-covariance">
<span class="eqno">(5)<a class="headerlink" href="#equation-spatial-covariance" title="Link to this equation">#</a></span>\[\mathbf{C}_{\text{spatial}} = \frac{1}{n_T - 1} \mathbf{X} \mathbf{X}^\top \in \mathbb{R}^{n_C \times n_C}\]</div>
<p><strong>Use cases</strong>:
* EEG motor imagery classification
* Brain-computer interfaces
* Spatial connectivity analysis</p>
</section>
</section>
<section id="example-spatial-covariance">
<h2>Example: Spatial Covariance<a class="headerlink" href="#example-spatial-covariance" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>

<span class="c1"># Simulate EEG data: 32 channels, 1000 time points</span>
<span class="n">n_channels</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n_timepoints</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">n_timepoints</span><span class="p">)</span>

<span class="c1"># Center the data (zero mean)</span>
<span class="n">X_centered</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Compute spatial covariance</span>
<span class="n">C_spatial</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_centered</span> <span class="o">@</span> <span class="n">X_centered</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_timepoints</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Spatial covariance shape: </span><span class="si">{</span><span class="n">C_spatial</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># (32, 32)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Is SPD: </span><span class="si">{</span><span class="n">is_spd</span><span class="p">(</span><span class="n">C_spatial</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="temporal-covariance-matrix">
<h3>Temporal Covariance Matrix<a class="headerlink" href="#temporal-covariance-matrix" title="Link to this heading">#</a></h3>
<p>The <strong>temporal covariance matrix</strong> captures correlations across time points for a given spatial
configuration.</p>
<div class="math notranslate nohighlight" id="equation-temporal-covariance">
<span class="eqno">(6)<a class="headerlink" href="#equation-temporal-covariance" title="Link to this equation">#</a></span>\[\mathbf{C}_{\text{temporal}} = \frac{1}{n_C - 1} \mathbf{X}^\top \mathbf{X} \in \mathbb{R}^{n_T \times n_T}\]</div>
<p><strong>Use cases</strong>:
* Time-series analysis
* Temporal dynamics
* State transitions</p>
<p><strong>Note</strong>: Temporal covariance matrices can become very large when <span class="math notranslate nohighlight">\(n_T\)</span> is large,
making them computationally expensive.</p>
</section>
<section id="cross-spectral-density-csd-matrix">
<h3>Cross-Spectral Density (CSD) Matrix<a class="headerlink" href="#cross-spectral-density-csd-matrix" title="Link to this heading">#</a></h3>
<p>The <strong>cross-spectral density matrix</strong> captures frequency-specific spatial correlations. For complex
Fourier coefficients <span class="math notranslate nohighlight">\(\mathbf{Z}_f \in \mathbb{C}^{n_C}\)</span> at frequency <span class="math notranslate nohighlight">\(f\)</span>:</p>
<div class="math notranslate nohighlight" id="equation-csd-matrix">
<span class="eqno">(7)<a class="headerlink" href="#equation-csd-matrix" title="Link to this equation">#</a></span>\[\mathbf{C}_f = \Re\!\left( \mathbf{Z}_f \, \overline{\mathbf{Z}_f}^{\,\top} \right)\]</div>
<p>where <span class="math notranslate nohighlight">\(\overline{\mathbf{Z}_f}^{\,\top}\)</span> is the conjugate transpose.</p>
<p><strong>Use cases</strong>:
* Frequency-specific connectivity
* Motor imagery with specific frequency bands (alpha, beta, mu)
* Cross-frequency coupling analysis</p>
</section>
</section>
<section id="example-cross-spectral-density">
<h2>Example: Cross-Spectral Density<a class="headerlink" href="#example-cross-spectral-density" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">torch.fft</span><span class="w"> </span><span class="kn">import</span> <span class="n">fft</span>

<span class="c1"># Simulate EEG data</span>
<span class="n">n_channels</span> <span class="o">=</span> <span class="mi">32</span>
<span class="n">n_timepoints</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">fs</span> <span class="o">=</span> <span class="mi">250</span>  <span class="c1"># Sampling frequency</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">n_timepoints</span><span class="p">)</span>

<span class="c1"># Compute FFT</span>
<span class="n">X_fft</span> <span class="o">=</span> <span class="n">fft</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># Select alpha band (8-13 Hz)</span>
<span class="n">freqs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">fft</span><span class="o">.</span><span class="n">fftfreq</span><span class="p">(</span><span class="n">n_timepoints</span><span class="p">,</span> <span class="n">d</span><span class="o">=</span><span class="mi">1</span> <span class="o">/</span> <span class="n">fs</span><span class="p">)</span>
<span class="n">alpha_idx</span> <span class="o">=</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">&gt;=</span> <span class="mi">8</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">freqs</span> <span class="o">&lt;=</span> <span class="mi">13</span><span class="p">)</span>

<span class="c1"># Extract alpha band coefficients</span>
<span class="n">Z_alpha</span> <span class="o">=</span> <span class="n">X_fft</span><span class="p">[:,</span> <span class="n">alpha_idx</span><span class="p">]</span>  <span class="c1"># Shape: (n_channels, n_alpha_freqs)</span>

<span class="c1"># Compute CSD for alpha band (average across frequencies)</span>
<span class="n">C_alpha</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">n_channels</span><span class="p">)</span>
<span class="k">for</span> <span class="n">f_idx</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">Z_alpha</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
    <span class="n">Z_f</span> <span class="o">=</span> <span class="n">Z_alpha</span><span class="p">[:,</span> <span class="n">f_idx</span><span class="p">]</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (n_channels, 1)</span>
    <span class="c1"># C_f = Re(Z_f * conj(Z_f)^T)</span>
    <span class="n">C_f</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">real</span><span class="p">(</span><span class="n">Z_f</span> <span class="o">@</span> <span class="n">Z_f</span><span class="o">.</span><span class="n">conj</span><span class="p">()</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
    <span class="n">C_alpha</span> <span class="o">+=</span> <span class="n">C_f</span>

<span class="n">C_alpha</span> <span class="o">/=</span> <span class="n">Z_alpha</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>  <span class="c1"># Average</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Alpha-band CSD shape: </span><span class="si">{</span><span class="n">C_alpha</span><span class="o">.</span><span class="n">shape</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Is SPD: </span><span class="si">{</span><span class="n">is_spd</span><span class="p">(</span><span class="n">C_alpha</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="regularized-covariance-matrix">
<h3>Regularized Covariance Matrix<a class="headerlink" href="#regularized-covariance-matrix" title="Link to this heading">#</a></h3>
<p>When <span class="math notranslate nohighlight">\(n_T &lt; n_C\)</span> (more channels than samples), the empirical covariance matrix is <strong>singular</strong>
(rank-deficient) and only positive semi-definite, not positive definite.</p>
<p><strong>Regularization strategies</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Diagonal loading (shrinkage)</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-diagonal-loading">
<span class="eqno">(8)<a class="headerlink" href="#equation-diagonal-loading" title="Link to this equation">#</a></span>\[\mathbf{C}_{\text{reg}} = (1 - \alpha) \mathbf{C}_{\text{emp}} + \alpha \text{tr}(\mathbf{C}_{\text{emp}}) \mathbf{I}\]</div>
<ol class="arabic simple" start="2">
<li><p><strong>Ledoit-Wolf shrinkage</strong>:</p></li>
</ol>
<div class="math notranslate nohighlight" id="equation-ledoit-wolf">
<span class="eqno">(9)<a class="headerlink" href="#equation-ledoit-wolf" title="Link to this equation">#</a></span>\[\mathbf{C}_{\text{LW}} = (1 - \delta) \mathbf{C}_{\text{emp}} + \delta \mu \mathbf{I}\]</div>
<p>where <span class="math notranslate nohighlight">\(\delta\)</span> and <span class="math notranslate nohighlight">\(\mu\)</span> are estimated from the data.</p>
<ol class="arabic simple" start="3">
<li><p><strong>Oracle Approximating Shrinkage (OAS)</strong>:</p></li>
</ol>
<p>An improved shrinkage estimator that minimizes the Mean Squared Error under Gaussian assumptions.</p>
</section>
</section>
<section id="example-regularized-covariance">
<h2>Example: Regularized Covariance<a class="headerlink" href="#example-regularized-covariance" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.covariance</span><span class="w"> </span><span class="kn">import</span> <span class="n">LedoitWolf</span><span class="p">,</span> <span class="n">OAS</span>

<span class="c1"># Simulate underdetermined case: more channels than samples</span>
<span class="n">n_channels</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">n_timepoints</span> <span class="o">=</span> <span class="mi">30</span>  <span class="c1"># n_T &lt; n_C</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_channels</span><span class="p">,</span> <span class="n">n_timepoints</span><span class="p">)</span>
<span class="n">X_centered</span> <span class="o">=</span> <span class="n">X</span> <span class="o">-</span> <span class="n">X</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

<span class="c1"># Empirical covariance (singular!)</span>
<span class="n">C_emp</span> <span class="o">=</span> <span class="p">(</span><span class="n">X_centered</span> <span class="o">@</span> <span class="n">X_centered</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">n_timepoints</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">eigvals_emp</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C_emp</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Empirical: min eigenvalue = </span><span class="si">{</span><span class="n">eigvals_emp</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>  <span class="c1"># Will be ~0</span>

<span class="c1"># Method 1: Simple diagonal loading</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">C_diag</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="n">alpha</span><span class="p">)</span> <span class="o">*</span> <span class="n">C_emp</span> <span class="o">+</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">trace</span><span class="p">(</span><span class="n">C_emp</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_channels</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Diagonal loading: min eigenvalue = </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C_diag</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 2: Ledoit-Wolf shrinkage</span>
<span class="n">lw</span> <span class="o">=</span> <span class="n">LedoitWolf</span><span class="p">()</span>
<span class="n">C_lw</span> <span class="o">=</span> <span class="n">lw</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_centered</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">covariance_</span>
<span class="n">C_lw</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">C_lw</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ledoit-Wolf: min eigenvalue = </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C_lw</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Method 3: OAS</span>
<span class="n">oas</span> <span class="o">=</span> <span class="n">OAS</span><span class="p">()</span>
<span class="n">C_oas</span> <span class="o">=</span> <span class="n">oas</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_centered</span><span class="o">.</span><span class="n">T</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">covariance_</span>
<span class="n">C_oas</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">C_oas</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;OAS: min eigenvalue = </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C_oas</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
<section id="functional-connectivity-matrix-fmri">
<h3>Functional Connectivity Matrix (fMRI)<a class="headerlink" href="#functional-connectivity-matrix-fmri" title="Link to this heading">#</a></h3>
<p>For fMRI, the <strong>functional connectivity matrix</strong> represents correlations between BOLD signals
across different brain regions.</p>
<div class="math notranslate nohighlight" id="equation-functional-connectivity">
<span class="eqno">(10)<a class="headerlink" href="#equation-functional-connectivity" title="Link to this equation">#</a></span>\[\mathbf{C}_{ij} = \text{corr}(\text{BOLD}_i, \text{BOLD}_j)\]</div>
<p>where <span class="math notranslate nohighlight">\(\text{BOLD}_i\)</span> is the time series of region <span class="math notranslate nohighlight">\(i\)</span>.</p>
<p><strong>Computation steps</strong>:
1. Parcellate brain into regions (using atlases like AAL, Harvard-Oxford)
2. Extract average BOLD time series per region
3. Compute correlation or covariance matrix</p>
<p><strong>Use cases</strong>:
* Resting-state connectivity
* Task-based connectivity
* Disease biomarker identification (Alzheimer’s, schizophrenia)</p>
</section>
<section id="diffusion-tensor-dti">
<h3>Diffusion Tensor (DTI)<a class="headerlink" href="#diffusion-tensor-dti" title="Link to this heading">#</a></h3>
<p>In diffusion tensor imaging (DTI), each voxel contains a <strong>diffusion tensor</strong> that describes
water molecule diffusion:</p>
<div class="math notranslate nohighlight" id="equation-diffusion-tensor">
<span class="eqno">(11)<a class="headerlink" href="#equation-diffusion-tensor" title="Link to this equation">#</a></span>\[\begin{split}\mathbf{D} = \begin{bmatrix}
D_{xx} &amp; D_{xy} &amp; D_{xz} \\
D_{xy} &amp; D_{yy} &amp; D_{yz} \\
D_{xz} &amp; D_{yz} &amp; D_{zz}
\end{bmatrix} \in \mathcal{S}_{++}^3\end{split}\]</div>
<p><strong>Use cases</strong>:
* Fiber tractography
* White matter analysis
* Anatomical smoothing</p>
</section>
</section>
<section id="covariance-estimation-methods">
<h2>Covariance Estimation Methods<a class="headerlink" href="#covariance-estimation-methods" title="Link to this heading">#</a></h2>
<section id="primary-references-for-covariance-estimation">
<h3>Primary References for Covariance Estimation<a class="headerlink" href="#primary-references-for-covariance-estimation" title="Link to this heading">#</a></h3>
<dl class="simple">
<dt><strong>Empirical Covariance</strong>:</dt><dd><p>The standard unbiased estimator, suitable when <span class="math notranslate nohighlight">\(n_T \gg n_C\)</span>.</p>
</dd>
</dl>
<p><strong>Shrinkage Estimators</strong>:</p>
<ul class="simple">
<li><p><strong>Ledoit &amp; Wolf (2004)</strong>: “A well-conditioned estimator for large-dimensional covariance matrices”
- Automatic shrinkage parameter estimation
- Minimizes MSE under Gaussian assumptions</p></li>
<li><p><strong>Chen et al. (2010)</strong>: “Shrinkage algorithms for MMSE covariance estimation”
- Oracle Approximating Shrinkage (OAS)
- Improved finite-sample performance</p></li>
</ul>
<p><strong>Robust Estimators</strong>:</p>
<ul class="simple">
<li><p><strong>Tyler’s M-estimator</strong>: Robust to outliers and heavy-tailed distributions</p></li>
<li><p><strong>Minimum Covariance Determinant (MCD)</strong>: Identifies and downweights outliers</p></li>
</ul>
<p><strong>Structured Covariance</strong>:</p>
<ul class="simple">
<li><p><strong>Graphical LASSO</strong>: Sparse inverse covariance (precision) estimation</p></li>
<li><p><strong>Factor Analysis</strong>: Low-rank plus diagonal structure</p></li>
</ul>
<p>For more details, we recommend the scikit-learn documentation on covariance estimation,
<span class="target" id="scikit-learn-covariance-estimation-and-the-documentation-within-pyriemann-pyriemann-documentation-cite-pyriemann">scikit-learn covariance estimation`_, and the documentation within PyRiemann,
_`PyRiemann documentation`_ :cite:`pyriemann</span>, that present a more in-depth overview of covariance estimation.</p>
</section>
</section>
<section id="example-comparing-estimators">
<h2>Example: Comparing Estimators<a class="headerlink" href="#example-comparing-estimators" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">sklearn.covariance</span><span class="w"> </span><span class="kn">import</span> <span class="n">EmpiricalCovariance</span><span class="p">,</span> <span class="n">LedoitWolf</span><span class="p">,</span> <span class="n">OAS</span><span class="p">,</span> <span class="n">MinCovDet</span>

<span class="c1"># Generate data with known covariance</span>
<span class="n">n_samples</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_features</span> <span class="o">=</span> <span class="mi">50</span>

<span class="c1"># True covariance (for comparison)</span>
<span class="n">true_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span> <span class="o">+</span> <span class="mf">0.5</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">)</span>
<span class="n">true_cov</span> <span class="o">=</span> <span class="n">true_cov</span> <span class="o">/</span> <span class="n">true_cov</span><span class="o">.</span><span class="n">diag</span><span class="p">()</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span>  <span class="c1"># Normalize</span>

<span class="c1"># Generate data</span>
<span class="n">L</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">true_cov</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="p">(</span><span class="n">L</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_samples</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>

<span class="c1"># Different estimators</span>
<span class="n">estimators</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s2">&quot;Empirical&quot;</span><span class="p">:</span> <span class="n">EmpiricalCovariance</span><span class="p">(),</span>
    <span class="s2">&quot;Ledoit-Wolf&quot;</span><span class="p">:</span> <span class="n">LedoitWolf</span><span class="p">(),</span>
    <span class="s2">&quot;OAS&quot;</span><span class="p">:</span> <span class="n">OAS</span><span class="p">(),</span>
    <span class="s2">&quot;MCD (robust)&quot;</span><span class="p">:</span> <span class="n">MinCovDet</span><span class="p">(),</span>
<span class="p">}</span>

<span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">estimator</span> <span class="ow">in</span> <span class="n">estimators</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="n">est_cov</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">estimator</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">numpy</span><span class="p">())</span><span class="o">.</span><span class="n">covariance_</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
    <span class="n">error</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">norm</span><span class="p">(</span><span class="n">est_cov</span> <span class="o">-</span> <span class="n">true_cov</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="s2">&quot;fro&quot;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">:</span><span class="s2">15s</span><span class="si">}</span><span class="s2">: Frobenius error = </span><span class="si">{</span><span class="n">error</span><span class="si">:</span><span class="s2">.4f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="parallel-transport-of-covariance-matrices">
<h2>Parallel Transport of Covariance Matrices<a class="headerlink" href="#parallel-transport-of-covariance-matrices" title="Link to this heading">#</a></h2>
<p><strong>Parallel transport</strong> is a fundamental operation in Riemannian geometry that moves tangent vectors
between different tangent spaces while preserving their geometric properties.</p>
<section id="why-parallel-transport-matters">
<h3>Why Parallel Transport Matters<a class="headerlink" href="#why-parallel-transport-matters" title="Link to this heading">#</a></h3>
<p>When working with SPD matrices from different sessions, subjects, or conditions, we often need to:</p>
<ol class="arabic simple">
<li><p><strong>Compare tangent vectors</strong> at different base points</p></li>
<li><p><strong>Transfer information</strong> from one covariance matrix to another</p></li>
<li><p><strong>Build consistent reference frames</strong> across the manifold</p></li>
</ol>
</section>
<section id="mathematical-formulation">
<h3>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h3>
<p>For the affine-invariant metric, parallel transport from <span class="math notranslate nohighlight">\(\mathbf{P}_1\)</span> to
<span class="math notranslate nohighlight">\(\mathbf{P}_2\)</span> of a tangent vector <span class="math notranslate nohighlight">\(\mathbf{S} \in T_{\mathbf{P}_1}\mathcal{M}\)</span> is:</p>
<div class="math notranslate nohighlight" id="equation-parallel-transport-formula">
<span class="eqno">(12)<a class="headerlink" href="#equation-parallel-transport-formula" title="Link to this equation">#</a></span>\[\Gamma_{\mathbf{P}_1 \to \mathbf{P}_2}(\mathbf{S}) = \mathbf{E} \, \mathbf{S} \, \mathbf{E}^\top\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{E} = \mathbf{P}_2^{1/2} \mathbf{P}_1^{-1/2}\)</span> is the parallel transport operator.</p>
<p><strong>Properties</strong>:
* Preserves inner products in tangent spaces
* Preserves symmetry of tangent vectors
* Composition: <span class="math notranslate nohighlight">\(\Gamma_{\mathbf{P}_1 \to \mathbf{P}_3} = \Gamma_{\mathbf{P}_2 \to \mathbf{P}_3} \circ \Gamma_{\mathbf{P}_1 \to \mathbf{P}_2}\)</span></p>
</section>
</section>
<section id="example-parallel-transport">
<h2>Example: Parallel Transport<a class="headerlink" href="#example-parallel-transport" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>
<span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">parallel_transport_airm</span>

<span class="c1"># Two base points (e.g., mean covariance from different sessions)</span>
<span class="n">P1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">2.0</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">1.5</span><span class="p">]])</span>
<span class="n">P2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.5</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">2.0</span><span class="p">]])</span>

<span class="c1"># Tangent vector at P1 (e.g., direction of maximum variance)</span>
<span class="n">S_at_P1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">]])</span>

<span class="c1"># Transport to P2</span>
<span class="n">S_at_P2</span> <span class="o">=</span> <span class="n">parallel_transport_airm</span><span class="p">(</span><span class="n">S_at_P1</span><span class="p">,</span> <span class="n">P1</span><span class="p">,</span> <span class="n">P2</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tangent vector at P1:</span><span class="se">\n</span><span class="si">{</span><span class="n">S_at_P1</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tangent vector at P2:</span><span class="se">\n</span><span class="si">{</span><span class="n">S_at_P2</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Verify: inner product is preserved</span>
<span class="c1"># (This requires the Riemannian metric, placeholder for now)</span>
<span class="c1"># inner_P1 = riemannian_inner_product(S_at_P1, S_at_P1, P1)</span>
<span class="c1"># inner_P2 = riemannian_inner_product(S_at_P2, S_at_P2, P2)</span>
<span class="c1"># print(f&quot;Inner product preserved: {torch.isclose(inner_P1, inner_P2)}&quot;)</span>
</pre></div>
</div>
<section id="optimal-parallel-transport">
<h3>Optimal Parallel Transport<a class="headerlink" href="#optimal-parallel-transport" title="Link to this heading">#</a></h3>
<p><strong>Optimal transport</strong> provides an alternative way to align covariance matrices by finding a
transport plan that minimizes a cost function. This differs from geometric parallel transport.</p>
<p>In the context of SPD matrices, optimal transport can be used to:</p>
<ul class="simple">
<li><p><strong>Match distributions</strong> of covariance matrices</p></li>
<li><p><strong>Domain adaptation</strong> across different recording sessions</p></li>
<li><p><strong>Transfer learning</strong> between subjects</p></li>
</ul>
<p><strong>Wasserstein distance</strong> on SPD manifold:</p>
<div class="math notranslate nohighlight" id="equation-wasserstein-spd">
<span class="eqno">(13)<a class="headerlink" href="#equation-wasserstein-spd" title="Link to this equation">#</a></span>\[W_2(\mathbf{P}_1, \mathbf{P}_2) = \left[\text{tr}(\mathbf{P}_1) + \text{tr}(\mathbf{P}_2) - 2\text{tr}\left(\mathbf{P}_1^{1/2}\mathbf{P}_2\mathbf{P}_1^{1/2}\right)^{1/2}\right]^{1/2}\]</div>
<p>This is also known as the <strong>Bures-Wasserstein metric</strong> and coincides with the affine-invariant
distance for SPD matrices.</p>
</section>
</section>
<section id="practical-considerations">
<h2>Practical Considerations<a class="headerlink" href="#practical-considerations" title="Link to this heading">#</a></h2>
<section id="sample-size-requirements">
<h3>Sample Size Requirements<a class="headerlink" href="#sample-size-requirements" title="Link to this heading">#</a></h3>
<p><strong>Rule of thumb</strong>: For stable covariance estimation without regularization:</p>
<div class="math notranslate nohighlight" id="equation-sample-requirement">
<span class="eqno">(14)<a class="headerlink" href="#equation-sample-requirement" title="Link to this equation">#</a></span>\[n_T \geq 2 \cdot n_C\]</div>
<p>When <span class="math notranslate nohighlight">\(n_T &lt; n_C\)</span>, regularization is <strong>essential</strong>.</p>
</section>
<section id="numerical-stability">
<h3>Numerical Stability<a class="headerlink" href="#numerical-stability" title="Link to this heading">#</a></h3>
<p><strong>Common issues</strong>:
1. <strong>Ill-conditioning</strong>: Very small eigenvalues due to noise
2. <strong>Numerical errors</strong>: Accumulated floating-point errors
3. <strong>Boundary proximity</strong>: Near-singular matrices</p>
<p><strong>Solutions</strong>:
* Eigenvalue clipping: <span class="math notranslate nohighlight">\(\lambda_i \leftarrow \max(\lambda_i, \epsilon)\)</span>
* Regularization: Add small constant to diagonal
* Use robust estimators for noisy data</p>
</section>
</section>
<section id="example-ensuring-numerical-stability">
<h2>Example: Ensuring Numerical Stability<a class="headerlink" href="#example-ensuring-numerical-stability" title="Link to this heading">#</a></h2>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">torch</span>


<span class="k">def</span><span class="w"> </span><span class="nf">ensure_spd</span><span class="p">(</span><span class="n">C</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-6</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Ensure matrix is SPD by eigenvalue clipping.&quot;&quot;&quot;</span>
    <span class="c1"># Symmetrize (handle numerical errors)</span>
    <span class="n">C_sym</span> <span class="o">=</span> <span class="p">(</span><span class="n">C</span> <span class="o">+</span> <span class="n">C</span><span class="o">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="mi">2</span>

    <span class="c1"># Eigenvalue decomposition</span>
    <span class="n">eigvals</span><span class="p">,</span> <span class="n">eigvecs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigh</span><span class="p">(</span><span class="n">C_sym</span><span class="p">)</span>

    <span class="c1"># Clip small/negative eigenvalues</span>
    <span class="n">eigvals_clipped</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">eigvals</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="n">epsilon</span><span class="p">)</span>

    <span class="c1"># Reconstruct</span>
    <span class="n">C_spd</span> <span class="o">=</span> <span class="n">eigvecs</span> <span class="o">@</span> <span class="n">torch</span><span class="o">.</span><span class="n">diag</span><span class="p">(</span><span class="n">eigvals_clipped</span><span class="p">)</span> <span class="o">@</span> <span class="n">eigvecs</span><span class="o">.</span><span class="n">T</span>

    <span class="k">return</span> <span class="n">C_spd</span>


<span class="c1"># Example: Nearly singular matrix</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([[</span><span class="mf">1.0</span><span class="p">,</span> <span class="mf">0.99</span><span class="p">],</span> <span class="p">[</span><span class="mf">0.99</span><span class="p">,</span> <span class="mf">1.0</span><span class="p">]])</span>
<span class="n">C</span> <span class="o">=</span> <span class="n">A</span> <span class="o">@</span> <span class="n">A</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Might have very small eigenvalues</span>

<span class="n">C_stable</span> <span class="o">=</span> <span class="n">ensure_spd</span><span class="p">(</span><span class="n">C</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Original min eigenvalue: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Stabilized min eigenvalue: </span><span class="si">{</span><span class="n">torch</span><span class="o">.</span><span class="n">linalg</span><span class="o">.</span><span class="n">eigvalsh</span><span class="p">(</span><span class="n">C_stable</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span><span class="si">:</span><span class="s2">.2e</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="key-takeaways">
<h2>Key Takeaways<a class="headerlink" href="#key-takeaways" title="Link to this heading">#</a></h2>
<ul class="simple">
<li><p><strong>SPD matrices</strong> require strict positive eigenvalues; PSD matrices (with zero eigenvalues) need
special handling</p></li>
<li><p><strong>Spatial covariance</strong> captures channel correlations, essential for BCI and connectivity analysis</p></li>
<li><p><strong>Cross-spectral density</strong> provides frequency-specific covariance for oscillatory analysis</p></li>
<li><p><strong>Regularization</strong> is essential when <span class="math notranslate nohighlight">\(n_T &lt; n_C\)</span> (Ledoit-Wolf, OAS are recommended)</p></li>
<li><p><strong>Parallel transport</strong> enables comparison of tangent vectors across the manifold</p></li>
<li><p><strong>Numerical stability</strong> requires eigenvalue clipping or regularization in practice</p></li>
<li><p>Choose covariance type and estimation method based on your data characteristics and analysis goals</p></li>
</ul>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<span class="target" id="id1"></span></section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="2_inherent_properties.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Inherent Properties and Their Exploitation</p>
      </div>
    </a>
    <a class="right-next"
       href="4_limitations.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Limitations and When Not to Use SPD Approaches</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2024-2026, SPD Learn Developers.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>