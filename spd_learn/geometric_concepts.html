
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Geometric Concepts &#8212; SPD Learn 0.0.1 documentation</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="_static/graphviz.css?v=4ae1632d" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/plot_directive.css" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery.css?v=d2d258e8" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-binder.css?v=f4aeca0c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-dataframe.css?v=2082cf3c" />
    <link rel="stylesheet" type="text/css" href="_static/sg_gallery-rendered-html.css?v=1277b6f3" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=7073f910" />
  
  <!-- So that users can add custom icons -->
  <script src="_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="_static/documentation_options.js?v=5aa371e9"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'geometric_concepts';</script>
    <link rel="canonical" href="https://spd-learn.github.io/spd_learn/geometric_concepts.html" />
    <link rel="icon" href="_static/spd_learn.png"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Numerical Stability" href="numerical_stability.html" />
    <link rel="prev" title="Theory" href="theory.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="0.0" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/spd_learn.png" class="logo__image only-light" alt="SPD Learn Logo"/>
    <img src="_static/spd_learn.png" class="logo__image only-dark pst-js-only" alt="SPD Learn Logo"/>
  
  
    <p class="title logo__title"><strong>SPD</strong> Learn</p>
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="theory.html">
    Theory
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="background/index.html">
    Background
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="generated/auto_examples/index.html">
    Examples
  </a>
</li>

            <li class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button"
                data-bs-toggle="dropdown" aria-expanded="false"
                aria-controls="pst-nav-more-links">
                    More
                </button>
                <ul id="pst-nav-more-links" class="dropdown-menu">
                    
<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="faq.html">
    FAQ
  </a>
</li>


<li class=" ">
  <a class="nav-link dropdown-item nav-internal" href="contributing.html">
    Contributing
  </a>
</li>

                </ul>
            </li>
            
  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
        <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/spd-learn/spd_learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/spd_learn/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item ">
  <a class="nav-link nav-internal" href="installation.html">
    Installation
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="user_guide.html">
    User Guide
  </a>
</li>


<li class="nav-item current active">
  <a class="nav-link nav-internal" href="theory.html">
    Theory
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="api.html">
    API
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="background/index.html">
    Background
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="generated/auto_examples/index.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="faq.html">
    FAQ
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="contributing.html">
    Contributing
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
          <div class="navbar-item"><ul class="navbar-icon-links"
    aria-label="External Links">
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://github.com/spd-learn/spd_learn" title="GitHub" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-github fa-lg" aria-hidden="true"></i>
            <span class="sr-only">GitHub</span></a>
        </li>
        <li class="nav-item">
          
          
          
          
          
          
          
          
          <a href="https://pypi.org/project/spd_learn/" title="PyPI" class="nav-link pst-navbar-icon" rel="noopener" target="_blank" data-bs-toggle="tooltip" data-bs-placement="bottom"><i class="fa-brands fa-python fa-lg" aria-hidden="true"></i>
            <span class="sr-only">PyPI</span></a>
        </li>
</ul></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1 current active"><a class="current reference internal" href="#">Geometric Concepts</a></li>
<li class="toctree-l1"><a class="reference internal" href="numerical_stability.html">Numerical Stability</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">Glossary</a></li>
<li class="toctree-l1"><a class="reference internal" href="references.html">References</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="theory.html" class="nav-link">Theory</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis">Geometric Concepts</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section id="geometric-concepts">
<span id="id1"></span><h1>Geometric Concepts<a class="headerlink" href="#geometric-concepts" title="Link to this heading">#</a></h1>
<p>This page provides a comprehensive introduction to the geometric foundations
underlying Riemannian methods for EEG/MEG analysis and deep learning with
covariance matrices on the SPD (Symmetric Positive Definite) manifold.</p>
<nav class="contents local" id="contents">
<p class="topic-title">Contents</p>
<ul class="simple">
<li><p><a class="reference internal" href="#the-spd-manifold" id="id68">The SPD Manifold</a></p>
<ul>
<li><p><a class="reference internal" href="#what-is-an-spd-matrix" id="id69">What is an SPD Matrix?</a></p></li>
<li><p><a class="reference internal" href="#why-spd-matrices-form-a-manifold" id="id70">Why SPD Matrices Form a Manifold</a></p></li>
<li><p><a class="reference internal" href="#the-spd-cone-2x2-example" id="id71">The SPD Cone (2x2 Example)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#tangent-spaces-and-exponential-maps" id="id72">Tangent Spaces and Exponential Maps</a></p>
<ul>
<li><p><a class="reference internal" href="#tangent-space-at-a-point" id="id73">Tangent Space at a Point</a></p></li>
<li><p><a class="reference internal" href="#the-exponential-map" id="id74">The Exponential Map</a></p></li>
<li><p><a class="reference internal" href="#the-logarithmic-map" id="id75">The Logarithmic Map</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#riemannian-metrics-on-spd-manifolds" id="id76">Riemannian Metrics on SPD Manifolds</a></p>
<ul>
<li><p><a class="reference internal" href="#affine-invariant-riemannian-metric-airm" id="id77">Affine-Invariant Riemannian Metric (AIRM)</a></p></li>
<li><p><a class="reference internal" href="#log-euclidean-metric-lem" id="id78">Log-Euclidean Metric (LEM)</a></p></li>
<li><p><a class="reference internal" href="#bures-wasserstein-metric" id="id79">Bures-Wasserstein Metric</a></p></li>
<li><p><a class="reference internal" href="#log-cholesky-metric" id="id80">Log-Cholesky Metric</a></p></li>
<li><p><a class="reference internal" href="#metric-comparison-table" id="id81">Metric Comparison Table</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#parallel-transport" id="id82">Parallel Transport</a></p></li>
<li><p><a class="reference internal" href="#why-this-matters-for-eeg-analysis" id="id83">Why This Matters for EEG Analysis</a></p>
<ul>
<li><p><a class="reference internal" href="#practical-implications" id="id84">Practical Implications</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#spd-layer-visualizations" id="id85">SPD Layer Visualizations</a></p></li>
<li><p><a class="reference internal" href="#references" id="id86">References</a></p></li>
</ul>
</nav>
<section id="the-spd-manifold">
<h2><a class="toc-backref" href="#id68" role="doc-backlink">The SPD Manifold</a><a class="headerlink" href="#the-spd-manifold" title="Link to this heading">#</a></h2>
<section id="what-is-an-spd-matrix">
<h3><a class="toc-backref" href="#id69" role="doc-backlink">What is an SPD Matrix?</a><a class="headerlink" href="#what-is-an-spd-matrix" title="Link to this heading">#</a></h3>
<p>A <strong>Symmetric Positive Definite (SPD)</strong> matrix <span class="math notranslate nohighlight">\(X \in \mathbb{R}^{n \times n}\)</span>
satisfies two conditions:</p>
<ol class="arabic simple">
<li><p><strong>Symmetry</strong>: <span class="math notranslate nohighlight">\(X = X^\top\)</span></p></li>
<li><p><strong>Positive Definiteness</strong>: <span class="math notranslate nohighlight">\(z^\top X z &gt; 0\)</span> for all non-zero vectors <span class="math notranslate nohighlight">\(z \in \mathbb{R}^n\)</span></p></li>
</ol>
<p>Equivalently, an SPD matrix has all positive eigenvalues <span id="id2">[<a class="reference internal" href="references.html#id6" title="Rajendra Bhatia. Positive Definite Matrices. Princeton University Press, 2007. doi:10.1515/9781400827787.">Bhatia, 2007</a>]</span>. The set of all
<span class="math notranslate nohighlight">\(n \times n\)</span> SPD matrices is denoted <span class="math notranslate nohighlight">\(\mathcal{S}^n_{++}\)</span> or
<span class="math notranslate nohighlight">\(\text{Sym}^+_n\)</span>.</p>
<div class="admonition-key-insight admonition">
<p class="admonition-title">Key Insight</p>
<p>Covariance matrices of non-degenerate random variables are always SPD.
This is why SPD geometry is fundamental for analyzing EEG, fMRI, and
other multivariate signals.</p>
</div>
</section>
<section id="why-spd-matrices-form-a-manifold">
<h3><a class="toc-backref" href="#id70" role="doc-backlink">Why SPD Matrices Form a Manifold</a><a class="headerlink" href="#why-spd-matrices-form-a-manifold" title="Link to this heading">#</a></h3>
<p>SPD matrices do <strong>not</strong> form a vector space because:</p>
<ol class="arabic simple">
<li><p><strong>Not closed under subtraction</strong>: If <span class="math notranslate nohighlight">\(A, B\)</span> are SPD, <span class="math notranslate nohighlight">\(A - B\)</span>
may not be SPD (positive definiteness can be violated).</p></li>
<li><p><strong>Not closed under negative scaling</strong>: If <span class="math notranslate nohighlight">\(A\)</span> is SPD,
<span class="math notranslate nohighlight">\(-A\)</span> is negative definite.</p></li>
<li><p><strong>The “swelling effect”</strong>: The Euclidean mean of SPD matrices can have
larger determinant than any of the original matrices, which is geometrically
undesirable for covariance estimation.</p></li>
</ol>
<p>Instead, SPD matrices form an <strong>open cone</strong> in the space of symmetric matrices.
This cone has a natural Riemannian manifold structure with well-defined
notions of distance, geodesics, and curvature.</p>
</section>
<section id="the-spd-cone-2x2-example">
<h3><a class="toc-backref" href="#id71" role="doc-backlink">The SPD Cone (2x2 Example)</a><a class="headerlink" href="#the-spd-cone-2x2-example" title="Link to this heading">#</a></h3>
<p>For <span class="math notranslate nohighlight">\(2 \times 2\)</span> matrices, we can visualize the SPD cone. A symmetric
<span class="math notranslate nohighlight">\(2 \times 2\)</span> matrix has three free parameters:</p>
<div class="math notranslate nohighlight">
\[\begin{split}X = \begin{pmatrix} a &amp; b \\ b &amp; c \end{pmatrix}\end{split}\]</div>
<p>The positive definiteness constraints are:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(a &gt; 0\)</span> (first leading minor)</p></li>
<li><p><span class="math notranslate nohighlight">\(ac - b^2 &gt; 0\)</span> (determinant, second leading minor)</p></li>
</ul>
<p>This defines an open cone in <span class="math notranslate nohighlight">\((a, b, c)\)</span> space, where the boundary
corresponds to singular (rank-deficient) matrices.</p>
<p>The interactive visualization below shows the SPD cone with sample EEG covariance
matrices plotted as points. The identity matrix serves as a reference point,
and the tangent space at the identity (the space of symmetric matrices) is shown
as a plane.</p>
<iframe src="_static/spd_manifold_eeg.html" width="100%" height="600px" style="border:none;"></iframe><div class="admonition tip">
<p class="admonition-title">Tip</p>
<p>Use your mouse to rotate, zoom, and explore the 3D visualization. Hover over
points to see their details.</p>
</div>
</section>
</section>
<section id="tangent-spaces-and-exponential-maps">
<h2><a class="toc-backref" href="#id72" role="doc-backlink">Tangent Spaces and Exponential Maps</a><a class="headerlink" href="#tangent-spaces-and-exponential-maps" title="Link to this heading">#</a></h2>
<section id="tangent-space-at-a-point">
<h3><a class="toc-backref" href="#id73" role="doc-backlink">Tangent Space at a Point</a><a class="headerlink" href="#tangent-space-at-a-point" title="Link to this heading">#</a></h3>
<p>At any point <span class="math notranslate nohighlight">\(P\)</span> on the SPD manifold, the <strong>tangent space</strong>
<span class="math notranslate nohighlight">\(T_P \mathcal{M}\)</span> is the set of symmetric matrices. This is a vector
space where we can perform standard linear algebra operations.</p>
<div class="math notranslate nohighlight">
\[T_P \mathcal{S}^n_{++} = \text{Sym}_n = \{ S \in \mathbb{R}^{n \times n} : S = S^\top \}\]</div>
<p>The tangent space at the identity <span class="math notranslate nohighlight">\(I\)</span> is particularly important because
many operations are simplified there.</p>
</section>
<section id="the-exponential-map">
<h3><a class="toc-backref" href="#id74" role="doc-backlink">The Exponential Map</a><a class="headerlink" href="#the-exponential-map" title="Link to this heading">#</a></h3>
<p>The <strong>exponential map</strong> <span class="math notranslate nohighlight">\(\text{Exp}_P: T_P\mathcal{M} \to \mathcal{M}\)</span>
projects tangent vectors back onto the manifold. At the identity:</p>
<div class="math notranslate nohighlight">
\[\text{Exp}_I(S) = \exp(S)\]</div>
<p>where <span class="math notranslate nohighlight">\(\exp\)</span> is the matrix exponential. This maps any symmetric matrix
to an SPD matrix, ensuring we stay on the manifold.</p>
</section>
<section id="the-logarithmic-map">
<h3><a class="toc-backref" href="#id75" role="doc-backlink">The Logarithmic Map</a><a class="headerlink" href="#the-logarithmic-map" title="Link to this heading">#</a></h3>
<p>The <strong>logarithmic map</strong> <span class="math notranslate nohighlight">\(\text{Log}_P: \mathcal{M} \to T_P\mathcal{M}\)</span>
is the inverse, projecting from the manifold to the tangent space:</p>
<div class="math notranslate nohighlight">
\[\text{Log}_I(X) = \log(X)\]</div>
<p>This is the key operation in SPD Learn’s <code class="docutils literal notranslate"><span class="pre">LogEig</span></code> layer, which maps SPD
matrices to a vector space for classification.</p>
</section>
</section>
<section id="riemannian-metrics-on-spd-manifolds">
<h2><a class="toc-backref" href="#id76" role="doc-backlink">Riemannian Metrics on SPD Manifolds</a><a class="headerlink" href="#riemannian-metrics-on-spd-manifolds" title="Link to this heading">#</a></h2>
<p>A <strong>Riemannian metric</strong> defines inner products on tangent spaces, enabling
us to measure distances and angles on the manifold. Several metrics are
commonly used for SPD matrices, each with different properties.</p>
<section id="affine-invariant-riemannian-metric-airm">
<h3><a class="toc-backref" href="#id77" role="doc-backlink">Affine-Invariant Riemannian Metric (AIRM)</a><a class="headerlink" href="#affine-invariant-riemannian-metric-airm" title="Link to this heading">#</a></h3>
<p>The <strong>Affine-Invariant Riemannian Metric</strong> <span id="id3">[<a class="reference internal" href="references.html#id3" title="Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A riemannian framework for tensor computing. International Journal of Computer Vision, 66(1):41–66, 2006. doi:10.1007/s11263-005-3222-z.">Pennec <em>et al.</em>, 2006</a>]</span> is the most natural metric for SPD
matrices. At point <span class="math notranslate nohighlight">\(P\)</span>, it is defined as:</p>
<div class="math notranslate nohighlight">
\[\langle S_1, S_2 \rangle_P = \text{tr}(P^{-1} S_1 P^{-1} S_2)\]</div>
<p><strong>Geodesic distance:</strong></p>
<div class="math notranslate nohighlight">
\[d_{\text{AIRM}}(A, B) = \| \log(A^{-1/2} B A^{-1/2}) \|_F\]</div>
<p><strong>Geodesic (shortest path):</strong></p>
<div class="math notranslate nohighlight">
\[\gamma(t) = A^{1/2} (A^{-1/2} B A^{-1/2})^t A^{1/2}\]</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Geodesically complete (geodesics exist for all time)</p></li>
<li><p>Affine-invariant: <span class="math notranslate nohighlight">\(d(GAG^\top, GBG^\top) = d(A, B)\)</span> for invertible <span class="math notranslate nohighlight">\(G\)</span></p></li>
<li><p>Computationally expensive (requires eigendecomposition)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">geodesic_distance_spdairm</span><span class="p">,</span>
    <span class="n">geodesic_interpolation_spdairm</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Distance between SPD matrices</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">geodesic_distance_spdairm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># Geodesic interpolation (t=0 gives A, t=1 gives B)</span>
<span class="n">midpoint</span> <span class="o">=</span> <span class="n">geodesic_interpolation_spdairm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="mf">0.5</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="log-euclidean-metric-lem">
<h3><a class="toc-backref" href="#id78" role="doc-backlink">Log-Euclidean Metric (LEM)</a><a class="headerlink" href="#log-euclidean-metric-lem" title="Link to this heading">#</a></h3>
<p>The <strong>Log-Euclidean Metric</strong> <span id="id4">[<a class="reference internal" href="references.html#id4" title="Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector space structure on symmetric positive-definite matrices. SIAM Journal on Matrix Analysis and Applications, 29(1):328–347, 2007. doi:10.1137/050637996.">Arsigny <em>et al.</em>, 2007</a>]</span> treats the logarithm of SPD matrices as vectors
in Euclidean space:</p>
<div class="math notranslate nohighlight">
\[d_{\text{LE}}(A, B) = \| \log(A) - \log(B) \|_F\]</div>
<p><strong>Fréchet mean:</strong></p>
<div class="math notranslate nohighlight">
\[\bar{X} = \exp\left( \frac{1}{n} \sum_{i=1}^n \log(X_i) \right)\]</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Gives SPD matrices a Lie group structure</p></li>
<li><p>Computationally efficient (one eigendecomposition per matrix)</p></li>
<li><p>Almost as good as AIRM for most practical applications</p></li>
<li><p>Default metric in SPD Learn</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="p">(</span>
    <span class="n">log_euclidean_distance</span><span class="p">,</span>
    <span class="n">log_euclidean_mean</span><span class="p">,</span>
    <span class="n">frechet_mean</span><span class="p">,</span>
<span class="p">)</span>

<span class="c1"># Distance</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">log_euclidean_distance</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># Mean of a batch</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">frechet_mean</span><span class="p">(</span><span class="n">batch_of_spd_matrices</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="bures-wasserstein-metric">
<h3><a class="toc-backref" href="#id79" role="doc-backlink">Bures-Wasserstein Metric</a><a class="headerlink" href="#bures-wasserstein-metric" title="Link to this heading">#</a></h3>
<p>The <strong>Bures-Wasserstein</strong> <span id="id5">[<a class="reference internal" href="references.html#id17" title="Suvrit Sra. Positive definite matrices and the s-divergence. Proceedings of the American Mathematical Society, 144(7):2787–2797, 2016. doi:10.1090/proc/12953.">Sra, 2016</a>, <a class="reference internal" href="references.html#id18" title="Donald Bures. An extension of kakutani's theorem on infinite product measures to the tensor product of semifinite w*-algebras. Transactions of the American Mathematical Society, 135:199–212, 1969. doi:10.1090/S0002-9947-1969-0236719-2.">Bures, 1969</a>]</span> (or Bures) metric comes from optimal transport theory:</p>
<div class="math notranslate nohighlight">
\[d_{\text{BW}}(A, B)^2 = \text{tr}(A) + \text{tr}(B) - 2\text{tr}\left((A^{1/2} B A^{1/2})^{1/2}\right)\]</div>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Better for ill-conditioned matrices</p></li>
<li><p>Related to quantum fidelity</p></li>
<li><p>Growing popularity in recent literature</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">bures_wasserstein_distance</span><span class="p">,</span> <span class="n">bures_wasserstein_mean</span>

<span class="c1"># Distance</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">bures_wasserstein_distance</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>

<span class="c1"># Barycenter (fixed-point iteration)</span>
<span class="n">mean</span> <span class="o">=</span> <span class="n">bures_wasserstein_mean</span><span class="p">(</span><span class="n">matrices</span><span class="p">,</span> <span class="n">weights</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="log-cholesky-metric">
<h3><a class="toc-backref" href="#id80" role="doc-backlink">Log-Cholesky Metric</a><a class="headerlink" href="#log-cholesky-metric" title="Link to this heading">#</a></h3>
<p>The <strong>Log-Cholesky</strong> metric uses the Cholesky decomposition for efficiency:</p>
<div class="math notranslate nohighlight">
\[d_{\text{LC}}(A, B) = \| \text{logchol}(L_A) - \text{logchol}(L_B) \|_F\]</div>
<p>where <span class="math notranslate nohighlight">\(L_A\)</span> is the Cholesky factor of <span class="math notranslate nohighlight">\(A\)</span> and <span class="math notranslate nohighlight">\(\text{logchol}\)</span>
applies log to the diagonal entries.</p>
<p><strong>Properties:</strong></p>
<ul class="simple">
<li><p>Fastest to compute (no eigendecomposition)</p></li>
<li><p>Numerically stable</p></li>
<li><p>Good approximation to AIRM</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">log_cholesky_distance</span><span class="p">,</span> <span class="n">log_cholesky_mean</span>

<span class="c1"># Fast distance computation</span>
<span class="n">dist</span> <span class="o">=</span> <span class="n">log_cholesky_distance</span><span class="p">(</span><span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="metric-comparison-table">
<h3><a class="toc-backref" href="#id81" role="doc-backlink">Metric Comparison Table</a><a class="headerlink" href="#metric-comparison-table" title="Link to this heading">#</a></h3>
<div class="pst-scrollable-table-container"><table class="table">
<colgroup>
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
<col style="width: 20.0%" />
</colgroup>
<thead>
<tr class="row-odd"><th class="head"><p>Metric</p></th>
<th class="head"><p>Complexity</p></th>
<th class="head"><p>Invariance</p></th>
<th class="head"><p>Stability</p></th>
<th class="head"><p>Best For</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>AIRM</strong></p></td>
<td><p>High</p></td>
<td><p>Full affine</p></td>
<td><p>Good</p></td>
<td><p>Theoretical analysis</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Log-Euclidean</strong></p></td>
<td><p>Medium</p></td>
<td><p>Orthogonal (Rotation)</p></td>
<td><p>Good</p></td>
<td><p>General use (default)</p></td>
</tr>
<tr class="row-even"><td><p><strong>Bures-Wasserstein</strong></p></td>
<td><p>Medium</p></td>
<td><p>Unitary</p></td>
<td><p>Excellent</p></td>
<td><p>Ill-conditioned matrices</p></td>
</tr>
<tr class="row-odd"><td><p><strong>Log-Cholesky</strong></p></td>
<td><p>Low</p></td>
<td><p>None</p></td>
<td><p>Excellent</p></td>
<td><p>Speed-critical applications</p></td>
</tr>
</tbody>
</table>
</div>
</section>
</section>
<section id="parallel-transport">
<h2><a class="toc-backref" href="#id82" role="doc-backlink">Parallel Transport</a><a class="headerlink" href="#parallel-transport" title="Link to this heading">#</a></h2>
<p><strong>Parallel transport</strong> moves tangent vectors between different tangent spaces
while preserving their geometric properties. This is essential for:</p>
<ul class="simple">
<li><p>Domain adaptation (transferring learned representations)</p></li>
<li><p>Comparing tangent vectors at different reference points</p></li>
<li><p>Riemannian optimization algorithms</p></li>
</ul>
<p>Under AIRM, parallel transport from <span class="math notranslate nohighlight">\(T_P\mathcal{M}\)</span> to <span class="math notranslate nohighlight">\(T_Q\mathcal{M}\)</span>:</p>
<div class="math notranslate nohighlight">
\[\Gamma_{P \to Q}(V) = E \cdot V \cdot E^\top\]</div>
<p>where <span class="math notranslate nohighlight">\(E = (Q P^{-1})^{1/2}\)</span>.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span><span class="w"> </span><span class="nn">spd_learn.functional</span><span class="w"> </span><span class="kn">import</span> <span class="n">parallel_transport_airm</span>

<span class="c1"># Transport tangent vector V from T_P to T_Q</span>
<span class="n">V_transported</span> <span class="o">=</span> <span class="n">parallel_transport_airm</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">Q</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>Key concepts illustrated:</strong></p>
<ol class="arabic simple">
<li><p><strong>Manifold</strong> <span class="math notranslate nohighlight">\(\mathcal{M}\)</span> — The curved space where our data lives
(e.g., SPD matrices representing EEG covariance)</p></li>
<li><p><strong>Tangent Space</strong> <span class="math notranslate nohighlight">\(T_p\mathcal{M} \cong \mathbb{R}^n\)</span> — A flat
Euclidean approximation at point <span class="math notranslate nohighlight">\(p\)</span>, where standard optimization
algorithms can be applied</p></li>
<li><p><strong>Exponential Map</strong> <span class="math notranslate nohighlight">\(\phi_p\)</span> — Projects points from the tangent space
back onto the manifold</p></li>
<li><p><strong>Dynamic Update</strong> — When optimization moves too far from the base point,
we update: <span class="math notranslate nohighlight">\(p_{i+1} := \phi_{p_i}(y_{i,k})\)</span> and continue optimizing
in the new tangent space</p></li>
</ol>
</section>
<section id="why-this-matters-for-eeg-analysis">
<h2><a class="toc-backref" href="#id83" role="doc-backlink">Why This Matters for EEG Analysis</a><a class="headerlink" href="#why-this-matters-for-eeg-analysis" title="Link to this heading">#</a></h2>
<p>In EEG/MEG analysis, spatial covariance matrices are naturally SPD matrices.
Methods like:</p>
<ul class="simple">
<li><p><strong>MDM</strong> (Minimum Distance to Mean) classifier <span id="id6">[<a class="reference internal" href="references.html#id5" title="Alexandre Barachant, Stéphane Bonnet, Marco Congedo, and Christian Jutten. Multiclass brain–computer interface classification by riemannian geometry. IEEE Transactions on Biomedical Engineering, 59(4):920–928, 2012. doi:10.1109/TBME.2011.2172210.">Barachant <em>et al.</em>, 2012</a>]</span></p></li>
<li><p><strong>Tangent Space</strong> projection for machine learning</p></li>
<li><p><strong>SPDNet</strong> <span id="id7">[<a class="reference internal" href="references.html#id9" title="Zhiwu Huang and Luc Van Gool. A riemannian network for spd matrix learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 31, 2036–2042. 2017. URL: https://ojs.aaai.org/index.php/AAAI/article/view/10866.">Huang and Van Gool, 2017</a>]</span> and other geometric deep learning architectures</p></li>
</ul>
<p>all leverage the Riemannian geometry of the SPD manifold. Understanding
these concepts helps explain:</p>
<ul class="simple">
<li><p>Why tangent space methods work well for classification</p></li>
<li><p>How geometric deep learning layers preserve SPD structure</p></li>
<li><p>The connection between classical Riemannian methods and modern deep learning</p></li>
<li><p>Why domain adaptation methods like RPA use parallel transport</p></li>
</ul>
<section id="practical-implications">
<h3><a class="toc-backref" href="#id84" role="doc-backlink">Practical Implications</a><a class="headerlink" href="#practical-implications" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Distance computation</strong>: Use Log-Euclidean for speed, AIRM for theoretical
guarantees, Bures-Wasserstein for ill-conditioned matrices.</p></li>
<li><p><strong>Averaging</strong>: Always use geometric means (<code class="docutils literal notranslate"><span class="pre">frechet_mean</span></code>) instead of
arithmetic means for SPD matrices.</p></li>
<li><p><strong>Classification</strong>: Project to tangent space (<code class="docutils literal notranslate"><span class="pre">LogEig</span></code>) before applying
standard classifiers.</p></li>
<li><p><strong>Domain adaptation</strong>: Use parallel transport to align representations
across subjects or sessions.</p></li>
</ol>
</section>
</section>
<section id="spd-layer-visualizations">
<h2><a class="toc-backref" href="#id85" role="doc-backlink">SPD Layer Visualizations</a><a class="headerlink" href="#spd-layer-visualizations" title="Link to this heading">#</a></h2>
<p>Understanding how SPD network layers transform data on the manifold is crucial
for building intuition about geometric deep learning. The visualizations below
show each layer’s operation using 2x2 SPD matrices represented as ellipsoids.</p>
<p><strong>CovLayer</strong> — Transforms time series into SPD covariance matrices:</p>
<div class="math notranslate nohighlight">
\[\Sigma = \frac{1}{T-1} (X - \bar{X})(X - \bar{X})^T\]</div>
<p>See <a class="reference internal" href="generated/auto_examples/visualizations/plot_covlayer_animation.html#sphx-glr-generated-auto-examples-visualizations-plot-covlayer-animation-py"><span class="std std-ref">CovLayer Animation</span></a></p>
<p><strong>BiMap</strong> — Bilinear mapping that reduces/expands dimensionality:</p>
<div class="math notranslate nohighlight">
\[Y = W^T X W\]</div>
<p>where <span class="math notranslate nohighlight">\(W\)</span> is constrained to the Stiefel manifold (<span class="math notranslate nohighlight">\(W^T W = I\)</span>).
See <a class="reference internal" href="generated/auto_examples/visualizations/plot_bimap_animation.html#sphx-glr-generated-auto-examples-visualizations-plot-bimap-animation-py"><span class="std std-ref">BiMap Layer Animation</span></a></p>
<p><strong>ReEig</strong> — Eigenvalue rectification (ReLU for SPD matrices):</p>
<div class="math notranslate nohighlight">
\[\text{ReEig}(X) = U \max(\Lambda, \epsilon) U^T\]</div>
<p>See <a class="reference internal" href="generated/auto_examples/visualizations/plot_reeig_animation.html#sphx-glr-generated-auto-examples-visualizations-plot-reeig-animation-py"><span class="std std-ref">ReEig Layer Animation</span></a></p>
<p><strong>LogEig</strong> — Projects SPD matrices to the tangent space:</p>
<div class="math notranslate nohighlight">
\[\text{LogEig}(X) = U \log(\Lambda) U^T\]</div>
<p>See <a class="reference internal" href="generated/auto_examples/visualizations/plot_logeig_animation.html#sphx-glr-generated-auto-examples-visualizations-plot-logeig-animation-py"><span class="std std-ref">LogEig Layer Animation</span></a></p>
<p><strong>SPDBatchNorm</strong> — Riemannian batch normalization:</p>
<div class="math notranslate nohighlight">
\[\tilde{X}_i = \mathcal{G}^{-1/2} X_i \mathcal{G}^{-1/2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathcal{G}\)</span> is the Fréchet mean of the batch.
See <a class="reference internal" href="generated/auto_examples/visualizations/plot_batchnorm_animation.html#sphx-glr-generated-auto-examples-visualizations-plot-batchnorm-animation-py"><span class="std std-ref">SPD Batch Normalization Animation</span></a></p>
</section>
<section id="references">
<h2><a class="toc-backref" href="#id86" role="doc-backlink">References</a><a class="headerlink" href="#references" title="Link to this heading">#</a></h2>
<div class="docutils container" id="id8">
<div role="list" class="citation-list">
<div class="citation" id="id15" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id7">1</a><span class="fn-bracket">]</span></span>
<p>Zhiwu Huang and Luc Van Gool. A riemannian network for spd matrix learning. In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, volume 31, 2036–2042. 2017. URL: <a class="reference external" href="https://ojs.aaai.org/index.php/AAAI/article/view/10866">https://ojs.aaai.org/index.php/AAAI/article/view/10866</a>.</p>
</div>
<div class="citation" id="id12" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id2">2</a><span class="fn-bracket">]</span></span>
<p>Rajendra Bhatia. <em>Positive Definite Matrices</em>. Princeton University Press, 2007. <a class="reference external" href="https://doi.org/10.1515/9781400827787">doi:10.1515/9781400827787</a>.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id3">3</a><span class="fn-bracket">]</span></span>
<p>Xavier Pennec, Pierre Fillard, and Nicholas Ayache. A riemannian framework for tensor computing. <em>International Journal of Computer Vision</em>, 66(1):41–66, 2006. <a class="reference external" href="https://doi.org/10.1007/s11263-005-3222-z">doi:10.1007/s11263-005-3222-z</a>.</p>
</div>
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">4</a><span class="fn-bracket">]</span></span>
<p>Vincent Arsigny, Pierre Fillard, Xavier Pennec, and Nicholas Ayache. Geometric means in a novel vector space structure on symmetric positive-definite matrices. <em>SIAM Journal on Matrix Analysis and Applications</em>, 29(1):328–347, 2007. <a class="reference external" href="https://doi.org/10.1137/050637996">doi:10.1137/050637996</a>.</p>
</div>
<div class="citation" id="id23" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">5</a><span class="fn-bracket">]</span></span>
<p>Suvrit Sra. Positive definite matrices and the s-divergence. <em>Proceedings of the American Mathematical Society</em>, 144(7):2787–2797, 2016. <a class="reference external" href="https://doi.org/10.1090/proc/12953">doi:10.1090/proc/12953</a>.</p>
</div>
<div class="citation" id="id24" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">6</a><span class="fn-bracket">]</span></span>
<p>Donald Bures. An extension of kakutani's theorem on infinite product measures to the tensor product of semifinite w*-algebras. <em>Transactions of the American Mathematical Society</em>, 135:199–212, 1969. <a class="reference external" href="https://doi.org/10.1090/S0002-9947-1969-0236719-2">doi:10.1090/S0002-9947-1969-0236719-2</a>.</p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id6">7</a><span class="fn-bracket">]</span></span>
<p>Alexandre Barachant, Stéphane Bonnet, Marco Congedo, and Christian Jutten. Multiclass brain–computer interface classification by riemannian geometry. <em>IEEE Transactions on Biomedical Engineering</em>, 59(4):920–928, 2012. <a class="reference external" href="https://doi.org/10.1109/TBME.2011.2172210">doi:10.1109/TBME.2011.2172210</a>.</p>
</div>
</div>
</div>
<div class="admonition seealso">
<p class="admonition-title">See also</p>
<ul class="simple">
<li><p><a class="reference internal" href="generated/auto_examples/visualizations/index.html"><span class="doc">SPD Layer Visualizations</span></a> — All visualization examples</p></li>
<li><p><a class="reference internal" href="user_guide.html"><span class="doc">User Guide</span></a> — Getting started with SPD Learn</p></li>
<li><p><a class="reference internal" href="api.html"><span class="doc">API Reference</span></a> — API Reference for all geometric operations</p></li>
<li><p><a class="reference internal" href="faq.html"><span class="doc">Frequently Asked Questions</span></a> — Frequently asked questions</p></li>
<li><p><a class="reference internal" href="contributing.html"><span class="doc">Contributing</span></a> — Contributing to SPD Learn</p></li>
</ul>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="theory.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Theory</p>
      </div>
    </a>
    <a class="right-next"
       href="numerical_stability.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Numerical Stability</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2025-2026, SPD Learn Developers.
      <br/>
    
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>