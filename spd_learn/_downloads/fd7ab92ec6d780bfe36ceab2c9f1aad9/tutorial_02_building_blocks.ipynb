{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPD Learn Example\n",
    "# ==================\n",
    "#\n",
    "# First, install the required packages:\n",
    "\n",
    "!uv pip install -q spd_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n\n# Building Blocks of SPD Neural Networks\n\nThis tutorial provides a comprehensive walkthrough of the fundamental building\nblocks used in SPD (Symmetric Positive Definite) neural networks. We explore\nthe mathematical foundations and geometric intuitions behind each layer.\n   :depth: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to SPD Neural Networks\n\nTraditional neural networks operate on vectors in Euclidean space.\nHowever, many real-world signals (EEG, fMRI, radar) are better represented\nas covariance matrices, which lie on a curved Riemannian manifold.\n\nThe SPDNet :cite:p:`huang2017riemannian` pipeline transforms raw signals\nthrough a series of geometry-aware operations:\n\n```text\nRaw Signal --> Covariance --> BiMap --> ReEig --> LogEig --> Linear\n(n_chans, n_times)   (n, n)    (m, m)    (m, m)   (m*(m+1)/2)   (n_classes)\n```\nEach layer respects the geometric structure of SPD matrices, ensuring\nthat intermediate representations remain valid covariance matrices.\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from spd_learn.modules import BiMap, CovLayer, LogEig, ReEig, Shrinkage, TraceNorm\n",
    "\n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set default figure size\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 4)\n",
    "plt.rcParams[\"figure.dpi\"] = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The SPD Manifold: A Quick Primer\n\nA Symmetric Positive Definite (SPD) matrix is a symmetric matrix with\nall positive eigenvalues. The space of all $n \\times n$ SPD matrices,\ndenoted $\\mathcal{S}^n_{++}$, forms a Riemannian manifold.\n\nKey properties:\n\n- **Not a vector space**: The sum of two SPD matrices is SPD, but\n  scalar multiplication can break positive definiteness\n- **Curved geometry**: Straight lines (geodesics) curve through the space\n- **Cone structure**: SPD matrices form an open convex cone\n\nWe can visualize 2x2 SPD matrices as ellipses:\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def visualize_spd_as_ellipse(spd_matrix, ax, center=(0, 0), color=\"blue\", alpha=0.5):\n",
    "    \"\"\"Visualize a 2x2 SPD matrix as an ellipse.\"\"\"\n",
    "    eigvals, eigvecs = np.linalg.eigh(spd_matrix)\n",
    "    width = 2 * np.sqrt(eigvals[1])\n",
    "    height = 2 * np.sqrt(eigvals[0])\n",
    "    angle = np.degrees(np.arctan2(eigvecs[1, 1], eigvecs[0, 1]))\n",
    "\n",
    "    from matplotlib.patches import Ellipse\n",
    "\n",
    "    ellipse = Ellipse(\n",
    "        center,\n",
    "        width,\n",
    "        height,\n",
    "        angle=angle,\n",
    "        alpha=alpha,\n",
    "        facecolor=color,\n",
    "        edgecolor=\"black\",\n",
    "        linewidth=2,\n",
    "    )\n",
    "    ax.add_patch(ellipse)\n",
    "    return ellipse\n",
    "\n",
    "\n",
    "# Create some example SPD matrices using PyTorch\n",
    "spd_1 = torch.tensor([[2.0, 0.5], [0.5, 1.0]], dtype=torch.float64)\n",
    "spd_2 = torch.tensor([[1.0, -0.3], [-0.3, 1.5]], dtype=torch.float64)\n",
    "spd_3 = torch.tensor([[3.0, 1.0], [1.0, 2.0]], dtype=torch.float64)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
    "ax.set_xlim(-4, 4)\n",
    "ax.set_ylim(-4, 4)\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.axhline(y=0, color=\"k\", linewidth=0.5)\n",
    "ax.axvline(x=0, color=\"k\", linewidth=0.5)\n",
    "\n",
    "# Convert to numpy for plotting\n",
    "visualize_spd_as_ellipse(spd_1.numpy(), ax, center=(-1.5, 1.5), color=\"#3498db\")\n",
    "visualize_spd_as_ellipse(spd_2.numpy(), ax, center=(1.5, 1.5), color=\"#e74c3c\")\n",
    "visualize_spd_as_ellipse(spd_3.numpy(), ax, center=(0, -1.5), color=\"#2ecc71\")\n",
    "\n",
    "ax.set_title(\"SPD Matrices Visualized as Ellipses\", fontsize=14, fontweight=\"bold\")\n",
    "ax.text(-1.5, 2.8, \"SPD 1\", ha=\"center\", fontsize=11)\n",
    "ax.text(1.5, 2.8, \"SPD 2\", ha=\"center\", fontsize=11)\n",
    "ax.text(0, -3.0, \"SPD 3\", ha=\"center\", fontsize=11)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 1: Covariance Layer (CovLayer)\n\nThe first step in an SPD network is to compute covariance matrices\nfrom raw multivariate signals. Given a signal $X \\in \\mathbb{R}^{C \\times T}$\nwith C channels and T time samples, the sample covariance is:\n\n\\begin{align}\\Sigma = \\frac{1}{T-1} X X^T\\end{align}\n\nThis maps the signal from Euclidean space to the SPD manifold.\n\n**Input shape**: ``(batch, n_channels, n_times)``\n\n**Output shape**: ``(batch, n_channels, n_channels)``\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generate synthetic multivariate time series\n",
    "batch_size = 4\n",
    "n_channels = 8\n",
    "n_times = 100\n",
    "\n",
    "# Create correlated signals\n",
    "raw_signals = torch.randn(batch_size, n_channels, n_times)\n",
    "\n",
    "# Add some structure (channel correlations)\n",
    "mixing_matrix = torch.randn(n_channels, n_channels)\n",
    "raw_signals = torch.einsum(\"ij,bjt->bit\", mixing_matrix, raw_signals)\n",
    "\n",
    "print(\"Input (raw signals):\")\n",
    "print(f\"  Shape: {raw_signals.shape}\")\n",
    "print(f\"  Min: {raw_signals.min():.3f}, Max: {raw_signals.max():.3f}\")\n",
    "\n",
    "# Apply CovLayer\n",
    "cov_layer = CovLayer()\n",
    "covariances = cov_layer(raw_signals)\n",
    "\n",
    "print(\"\\nOutput (covariance matrices):\")\n",
    "print(f\"  Shape: {covariances.shape}\")\n",
    "print(f\"  Symmetric: {torch.allclose(covariances, covariances.transpose(-2, -1))}\")\n",
    "\n",
    "# Check positive definiteness\n",
    "eigvals = torch.linalg.eigvalsh(covariances)\n",
    "print(f\"  Min eigenvalue: {eigvals.min():.6f} (should be > 0 for SPD)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Covariance Computation\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Plot raw signal (first sample, first 3 channels)\n",
    "ax1 = axes[0]\n",
    "time = np.arange(n_times)\n",
    "for i in range(3):\n",
    "    ax1.plot(time, raw_signals[0, i, :].numpy(), label=f\"Channel {i+1}\", alpha=0.8)\n",
    "ax1.set_xlabel(\"Time samples\")\n",
    "ax1.set_ylabel(\"Amplitude\")\n",
    "ax1.set_title(\"Raw Signal (3 channels)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot covariance matrix as heatmap\n",
    "ax2 = axes[1]\n",
    "cov_np = covariances[0].numpy()\n",
    "im = ax2.imshow(cov_np, cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax2.set_title(\"Covariance Matrix\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Channel\")\n",
    "ax2.set_ylabel(\"Channel\")\n",
    "plt.colorbar(im, ax=ax2, shrink=0.8)\n",
    "\n",
    "# Plot eigenvalue spectrum\n",
    "ax3 = axes[2]\n",
    "eigvals_np = eigvals[0].numpy()\n",
    "ax3.bar(range(n_channels), sorted(eigvals_np, reverse=True), color=\"#3498db\", alpha=0.8)\n",
    "ax3.set_xlabel(\"Eigenvalue index\")\n",
    "ax3.set_ylabel(\"Eigenvalue\")\n",
    "ax3.set_title(\"Eigenvalue Spectrum\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.set_yscale(\"log\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 2: Regularization (Shrinkage & TraceNorm)\n\nSample covariance matrices can be ill-conditioned, especially when\nthe number of samples is small relative to the number of channels.\nRegularization improves numerical stability.\n\n**Shrinkage (Ledoit-Wolf)** :cite:p:`ledoit2004well`:\n\n\\begin{align}\\Sigma_{reg} = (1 - \\alpha) \\Sigma + \\alpha \\cdot \\mu \\cdot I\\end{align}\n\nwhere $\\alpha$ is the shrinkage coefficient and $\\mu$ is\nthe average eigenvalue (trace / n).\n\n**Trace Normalization:**\n\n\\begin{align}\\Sigma_{norm} = \\frac{\\Sigma}{\\text{trace}(\\Sigma)} + \\epsilon I\\end{align}\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Apply shrinkage regularization\n",
    "shrinkage = Shrinkage(n_chans=n_channels, init_shrinkage=0.5, learnable=True)\n",
    "cov_shrunk = shrinkage(covariances)\n",
    "\n",
    "# Apply trace normalization\n",
    "trace_norm = TraceNorm(epsilon=1e-5)\n",
    "cov_normalized = trace_norm(covariances)\n",
    "\n",
    "print(\"Regularization effects:\")\n",
    "print(f\"  Original condition number: {torch.linalg.cond(covariances[0]):.1f}\")\n",
    "print(f\"  After shrinkage: {torch.linalg.cond(cov_shrunk[0].detach()):.1f}\")\n",
    "print(f\"  After trace norm: {torch.linalg.cond(cov_normalized[0]):.1f}\")\n",
    "\n",
    "# Visualize eigenvalue spectra\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "eigvals_orig = torch.linalg.eigvalsh(covariances[0]).numpy()\n",
    "eigvals_shrunk = torch.linalg.eigvalsh(cov_shrunk[0].detach()).numpy()\n",
    "eigvals_norm = torch.linalg.eigvalsh(cov_normalized[0]).numpy()\n",
    "\n",
    "for ax, eigv, title, color in zip(\n",
    "    axes,\n",
    "    [eigvals_orig, eigvals_shrunk, eigvals_norm],\n",
    "    [\"Original\", \"After Shrinkage\", \"After Trace Norm\"],\n",
    "    [\"#3498db\", \"#e74c3c\", \"#2ecc71\"],\n",
    "):\n",
    "    ax.bar(range(n_channels), sorted(eigv, reverse=True), color=color, alpha=0.8)\n",
    "    ax.set_xlabel(\"Eigenvalue index\")\n",
    "    ax.set_ylabel(\"Eigenvalue\")\n",
    "    ax.set_title(title, fontsize=12, fontweight=\"bold\")\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.axhline(y=min(eigv), color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "    ax.text(\n",
    "        n_channels - 1,\n",
    "        min(eigv) * 1.5,\n",
    "        f\"min={min(eigv):.2e}\",\n",
    "        ha=\"right\",\n",
    "        fontsize=9,\n",
    "        color=\"red\",\n",
    "    )\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 3: BiMap Layer\n\nThe BiMap (Bilinear Mapping) layer performs dimensionality reduction\nwhile preserving the SPD structure. It applies a congruence transformation:\n\n\\begin{align}Y = W^T X W\\end{align}\n\nwhere $W \\in \\mathbb{R}^{n \\times m}$ is constrained to lie on the\n**Stiefel manifold** ($W^T W = I$).\n\n**Key properties:**\n\n- If X is SPD, then Y is also SPD\n- Reduces dimension from n x n to m x m\n- W is orthogonal, preventing information collapse\n\n**Input shape**: ``(batch, n, n)``\n\n**Output shape**: ``(batch, m, m)``\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create BiMap layer: reduce from 8x8 to 4x4\n",
    "bimap = BiMap(in_features=n_channels, out_features=4, parametrized=True)\n",
    "\n",
    "# Apply BiMap\n",
    "cov_reduced = bimap(covariances)\n",
    "\n",
    "print(\"BiMap transformation:\")\n",
    "print(f\"  Input shape: {covariances.shape}\")\n",
    "print(f\"  Output shape: {cov_reduced.shape}\")\n",
    "print(f\"  Weight matrix W shape: {bimap.weight.shape}\")\n",
    "\n",
    "# Verify orthogonality of W\n",
    "W = bimap.weight[0]  # Get first (only) weight matrix\n",
    "WtW = W.T @ W\n",
    "print(\"\\n  W^T W (should be identity):\")\n",
    "print(f\"  {WtW.detach().numpy().round(4)}\")\n",
    "\n",
    "# Verify output is still SPD\n",
    "eigvals_reduced = torch.linalg.eigvalsh(cov_reduced)\n",
    "print(\n",
    "    f\"\\n  Output eigenvalues (all > 0): {eigvals_reduced[0].detach().numpy().round(4)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the BiMap Transformation\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n",
    "\n",
    "# Input covariance\n",
    "ax1 = axes[0]\n",
    "im1 = ax1.imshow(covariances[0].numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax1.set_title(\"Input (8x8)\", fontsize=12, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Channel\")\n",
    "ax1.set_ylabel(\"Channel\")\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "# Weight matrix W\n",
    "ax2 = axes[1]\n",
    "W_np = bimap.weight[0].detach().numpy()\n",
    "im2 = ax2.imshow(W_np, cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax2.set_title(\"W (8x4, Stiefel)\", fontsize=12, fontweight=\"bold\")\n",
    "ax2.set_xlabel(\"Output dim\")\n",
    "ax2.set_ylabel(\"Input dim\")\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "\n",
    "# W^T W (should be identity)\n",
    "ax3 = axes[2]\n",
    "WtW_np = (W.T @ W).detach().numpy()\n",
    "im3 = ax3.imshow(WtW_np, cmap=\"RdBu_r\", aspect=\"auto\", vmin=-0.1, vmax=1.1)\n",
    "ax3.set_title(r\"$W^T W$ (Identity)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.colorbar(im3, ax=ax3, shrink=0.8)\n",
    "\n",
    "# Output covariance\n",
    "ax4 = axes[3]\n",
    "im4 = ax4.imshow(cov_reduced[0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax4.set_title(\"Output (4x4)\", fontsize=12, fontweight=\"bold\")\n",
    "ax4.set_xlabel(\"Channel\")\n",
    "ax4.set_ylabel(\"Channel\")\n",
    "plt.colorbar(im4, ax=ax4, shrink=0.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 4: ReEig Layer (Rectified Eigenvalues)\n\nThe ReEig layer introduces non-linearity while preserving the SPD property.\nIt applies a ReLU-like function to eigenvalues:\n\n\\begin{align}\\text{ReEig}(X) = U \\max(\\Lambda, \\epsilon) U^T\\end{align}\n\nwhere $X = U \\Lambda U^T$ is the eigendecomposition and\n$\\epsilon$ is a small threshold.\n\n**Geometric interpretation:**\n\n- Clamps small eigenvalues to $\\epsilon$\n- Prevents matrices from becoming singular\n- Analogous to ReLU in standard neural networks\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create ReEig layer\n",
    "reeig = ReEig(threshold=1e-4)\n",
    "\n",
    "# Create a matrix with some small eigenvalues for demonstration\n",
    "demo_eigvals = torch.tensor([2.0, 0.5, 0.01, 0.001])\n",
    "demo_eigvecs = torch.linalg.qr(torch.randn(4, 4))[0]\n",
    "demo_spd = demo_eigvecs @ torch.diag(demo_eigvals) @ demo_eigvecs.T\n",
    "demo_spd = demo_spd.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "# Apply ReEig\n",
    "demo_rectified = reeig(demo_spd)\n",
    "\n",
    "# Compare eigenvalues\n",
    "eigvals_before = torch.linalg.eigvalsh(demo_spd[0])\n",
    "eigvals_after = torch.linalg.eigvalsh(demo_rectified[0])\n",
    "\n",
    "print(\"ReEig transformation:\")\n",
    "print(\"  Threshold: 1e-4\")\n",
    "print(f\"  Eigenvalues before: {eigvals_before.numpy().round(6)}\")\n",
    "print(f\"  Eigenvalues after:  {eigvals_after.numpy().round(6)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the ReEig Non-linearity\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot the ReEig function\n",
    "ax1 = axes[0]\n",
    "x = np.linspace(0, 2.5, 200)\n",
    "epsilon = 0.3  # Larger threshold for visualization\n",
    "y_reeig = np.maximum(x, epsilon)\n",
    "\n",
    "ax1.plot(x, x, \"k--\", alpha=0.4, label=\"Identity (y=x)\", linewidth=2)\n",
    "ax1.plot(x, y_reeig, \"b-\", linewidth=3, label=f\"ReEig (eps={epsilon})\")\n",
    "ax1.fill_between(\n",
    "    [0, epsilon], [epsilon, epsilon], [0, 0], color=\"red\", alpha=0.15, label=\"Clamped\"\n",
    ")\n",
    "ax1.axhline(y=epsilon, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "ax1.axvline(x=epsilon, color=\"red\", linestyle=\"--\", alpha=0.5)\n",
    "\n",
    "# Mark the demo eigenvalues (scaled for visualization)\n",
    "scale = 1.0\n",
    "for i, (ev_before, ev_after) in enumerate(\n",
    "    zip(eigvals_before.numpy() * scale, eigvals_after.numpy() * scale)\n",
    "):\n",
    "    if ev_before < 2.5:\n",
    "        ax1.scatter(\n",
    "            [ev_before], [max(ev_before, epsilon)], s=100, zorder=5, edgecolors=\"black\"\n",
    "        )\n",
    "        if ev_before < epsilon:\n",
    "            ax1.plot(\n",
    "                [ev_before, ev_before],\n",
    "                [ev_before, epsilon],\n",
    "                \"r:\",\n",
    "                linewidth=2,\n",
    "                alpha=0.7,\n",
    "            )\n",
    "\n",
    "ax1.set_xlim(-0.1, 2.5)\n",
    "ax1.set_ylim(-0.1, 2.5)\n",
    "ax1.set_xlabel(\"Input eigenvalue\", fontsize=12)\n",
    "ax1.set_ylabel(\"Output eigenvalue\", fontsize=12)\n",
    "ax1.set_title(\"ReEig: Eigenvalue Rectification\", fontsize=13, fontweight=\"bold\")\n",
    "ax1.legend(loc=\"lower right\", fontsize=10)\n",
    "ax1.grid(True, alpha=0.3)\n",
    "ax1.set_aspect(\"equal\")\n",
    "\n",
    "# Bar plot of eigenvalues\n",
    "ax2 = axes[1]\n",
    "x_pos = np.arange(4)\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax2.bar(\n",
    "    x_pos - width / 2,\n",
    "    eigvals_before.numpy(),\n",
    "    width,\n",
    "    label=\"Before ReEig\",\n",
    "    color=\"#3498db\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "bars2 = ax2.bar(\n",
    "    x_pos + width / 2,\n",
    "    eigvals_after.numpy(),\n",
    "    width,\n",
    "    label=\"After ReEig\",\n",
    "    color=\"#e74c3c\",\n",
    "    alpha=0.8,\n",
    ")\n",
    "ax2.axhline(y=1e-4, color=\"green\", linestyle=\"--\", linewidth=2, label=\"Threshold\")\n",
    "ax2.set_xlabel(\"Eigenvalue index\", fontsize=12)\n",
    "ax2.set_ylabel(\"Eigenvalue\", fontsize=12)\n",
    "ax2.set_title(\"Eigenvalue Comparison\", fontsize=13, fontweight=\"bold\")\n",
    "ax2.set_yscale(\"log\")\n",
    "ax2.set_xticks(x_pos)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Block 5: LogEig Layer (Logarithmic Map)\n\nThe LogEig layer maps SPD matrices to the tangent space at the identity\nby applying the matrix logarithm:\n\n\\begin{align}\\log(X) = U \\log(\\Lambda) U^T\\end{align}\n\nwhere $X = U \\Lambda U^T$ is the eigendecomposition.\n\n**Geometric interpretation:**\n\n- Projects from curved manifold to flat tangent space\n- In tangent space, standard Euclidean operations apply\n- Output is a symmetric matrix, which can be vectorized\n\n**Key insight**: The logarithm \"flattens\" the SPD manifold, allowing\nus to use standard linear classifiers.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create LogEig layer with vectorization\n",
    "logeig = LogEig(upper=True)\n",
    "\n",
    "# Use our reduced covariances\n",
    "log_matrices = logeig(cov_reduced.detach())\n",
    "\n",
    "print(\"LogEig transformation:\")\n",
    "print(f\"  Input shape: {cov_reduced.shape}\")\n",
    "print(f\"  Output shape: {log_matrices.shape}\")\n",
    "print(f\"  Output dimension formula: n*(n+1)/2 = 4*5/2 = {4 * 5 // 2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understanding the Matrix Logarithm\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# Input SPD matrix\n",
    "ax1 = axes[0]\n",
    "spd_input = cov_reduced[0].detach().numpy()\n",
    "im1 = ax1.imshow(spd_input, cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax1.set_title(\"Input SPD Matrix\", fontsize=12, fontweight=\"bold\")\n",
    "plt.colorbar(im1, ax=ax1, shrink=0.8)\n",
    "\n",
    "# Compute full matrix log (without vectorization) for visualization\n",
    "logeig_full = LogEig(upper=False, flatten=False)\n",
    "log_full = logeig_full(cov_reduced.detach())\n",
    "\n",
    "# Matrix logarithm\n",
    "ax2 = axes[1]\n",
    "log_matrix = log_full[0].numpy()\n",
    "im2 = ax2.imshow(log_matrix, cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax2.set_title(r\"$\\log(X)$ (tangent space)\", fontsize=12, fontweight=\"bold\")\n",
    "plt.colorbar(im2, ax=ax2, shrink=0.8)\n",
    "\n",
    "# Vectorized output\n",
    "ax3 = axes[2]\n",
    "vec_output = log_matrices[0].numpy()\n",
    "ax3.bar(range(len(vec_output)), vec_output, color=\"#2ecc71\", alpha=0.8)\n",
    "ax3.set_xlabel(\"Vector index\", fontsize=11)\n",
    "ax3.set_ylabel(\"Value\", fontsize=11)\n",
    "ax3.set_title(\"Vectorized Output (upper triangular)\", fontsize=12, fontweight=\"bold\")\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete SPDNet Pipeline\n\nNow let's put it all together and trace the shape transformations\nthrough a complete SPDNet pipeline.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class SimpleSPDNet(nn.Module):\n",
    "    \"\"\"A simple SPD network for demonstration.\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        # Layer 1: Covariance computation\n",
    "        self.cov = CovLayer()\n",
    "\n",
    "        # Layer 2: Regularization\n",
    "        self.shrinkage = Shrinkage(n_chans=n_channels, init_shrinkage=0.1)\n",
    "\n",
    "        # Layer 3: BiMap (reduce dimension)\n",
    "        self.bimap1 = BiMap(in_features=n_channels, out_features=n_channels // 2)\n",
    "\n",
    "        # Layer 4: ReEig (non-linearity)\n",
    "        self.reeig1 = ReEig()\n",
    "\n",
    "        # Layer 5: Another BiMap\n",
    "        self.bimap2 = BiMap(in_features=n_channels // 2, out_features=n_channels // 4)\n",
    "\n",
    "        # Layer 6: Another ReEig\n",
    "        self.reeig2 = ReEig()\n",
    "\n",
    "        # Layer 7: LogEig (project to tangent space)\n",
    "        self.logeig = LogEig(upper=True)\n",
    "\n",
    "        # Layer 8: Linear classifier\n",
    "        tangent_dim = (n_channels // 4) * (n_channels // 4 + 1) // 2\n",
    "        self.classifier = nn.Linear(tangent_dim, n_classes)\n",
    "\n",
    "    def forward(self, x, return_intermediates=False):\n",
    "        intermediates = {}\n",
    "\n",
    "        # Raw signal -> Covariance\n",
    "        x = self.cov(x)\n",
    "        intermediates[\"cov\"] = x.clone()\n",
    "\n",
    "        # Shrinkage regularization\n",
    "        x = self.shrinkage(x)\n",
    "        intermediates[\"shrinkage\"] = x.clone()\n",
    "\n",
    "        # BiMap + ReEig block 1\n",
    "        x = self.bimap1(x)\n",
    "        intermediates[\"bimap1\"] = x.clone()\n",
    "        x = self.reeig1(x)\n",
    "        intermediates[\"reeig1\"] = x.clone()\n",
    "\n",
    "        # BiMap + ReEig block 2\n",
    "        x = self.bimap2(x)\n",
    "        intermediates[\"bimap2\"] = x.clone()\n",
    "        x = self.reeig2(x)\n",
    "        intermediates[\"reeig2\"] = x.clone()\n",
    "\n",
    "        # LogEig (to tangent space)\n",
    "        x = self.logeig(x)\n",
    "        intermediates[\"logeig\"] = x.clone()\n",
    "\n",
    "        # Linear classifier\n",
    "        x = self.classifier(x)\n",
    "        intermediates[\"output\"] = x.clone()\n",
    "\n",
    "        if return_intermediates:\n",
    "            return x, intermediates\n",
    "        return x\n",
    "\n",
    "\n",
    "# Create and use the network\n",
    "n_channels = 16\n",
    "n_classes = 4\n",
    "model = SimpleSPDNet(n_channels=n_channels, n_classes=n_classes)\n",
    "\n",
    "# Generate input\n",
    "batch_size = 8\n",
    "n_times = 200\n",
    "raw_input = torch.randn(batch_size, n_channels, n_times)\n",
    "\n",
    "# Forward pass with intermediates\n",
    "output, intermediates = model(raw_input, return_intermediates=True)\n",
    "\n",
    "# Print shape transformations\n",
    "print(\"Shape Transformations Through SPDNet\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Input (raw signal):     {raw_input.shape}\")\n",
    "print(f\"After CovLayer:         {intermediates['cov'].shape}\")\n",
    "print(f\"After Shrinkage:        {intermediates['shrinkage'].shape}\")\n",
    "print(f\"After BiMap1 (16->8):   {intermediates['bimap1'].shape}\")\n",
    "print(f\"After ReEig1:           {intermediates['reeig1'].shape}\")\n",
    "print(f\"After BiMap2 (8->4):    {intermediates['bimap2'].shape}\")\n",
    "print(f\"After ReEig2:           {intermediates['reeig2'].shape}\")\n",
    "print(f\"After LogEig:           {intermediates['logeig'].shape}\")\n",
    "print(f\"Output (logits):        {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing the Full Pipeline\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "\n",
    "# Row 1: Matrix representations\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(raw_input[0, :3, :].T.numpy(), alpha=0.7)\n",
    "ax1.set_title(\"Raw Signal\", fontsize=11, fontweight=\"bold\")\n",
    "ax1.set_xlabel(\"Time\")\n",
    "ax1.set_ylabel(\"Amplitude\")\n",
    "\n",
    "ax2 = axes[0, 1]\n",
    "im2 = ax2.imshow(intermediates[\"cov\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\")\n",
    "ax2.set_title(\"Covariance (16x16)\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax3 = axes[0, 2]\n",
    "im3 = ax3.imshow(\n",
    "    intermediates[\"bimap1\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\"\n",
    ")\n",
    "ax3.set_title(\"After BiMap1 (8x8)\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "ax4 = axes[0, 3]\n",
    "im4 = ax4.imshow(\n",
    "    intermediates[\"bimap2\"][0].detach().numpy(), cmap=\"RdBu_r\", aspect=\"auto\"\n",
    ")\n",
    "ax4.set_title(\"After BiMap2 (4x4)\", fontsize=11, fontweight=\"bold\")\n",
    "\n",
    "# Row 2: Eigenvalue spectra and output\n",
    "ax5 = axes[1, 0]\n",
    "eigvals_cov = torch.linalg.eigvalsh(intermediates[\"cov\"][0]).numpy()\n",
    "ax5.bar(range(len(eigvals_cov)), sorted(eigvals_cov, reverse=True), color=\"#3498db\")\n",
    "ax5.set_title(\"Cov Eigenvalues\", fontsize=11, fontweight=\"bold\")\n",
    "ax5.set_yscale(\"log\")\n",
    "\n",
    "ax6 = axes[1, 1]\n",
    "eigvals_bimap1 = torch.linalg.eigvalsh(intermediates[\"reeig1\"][0]).detach().numpy()\n",
    "ax6.bar(\n",
    "    range(len(eigvals_bimap1)), sorted(eigvals_bimap1, reverse=True), color=\"#e74c3c\"\n",
    ")\n",
    "ax6.set_title(\"After ReEig1\", fontsize=11, fontweight=\"bold\")\n",
    "ax6.set_yscale(\"log\")\n",
    "\n",
    "ax7 = axes[1, 2]\n",
    "eigvals_bimap2 = torch.linalg.eigvalsh(intermediates[\"reeig2\"][0]).detach().numpy()\n",
    "ax7.bar(\n",
    "    range(len(eigvals_bimap2)), sorted(eigvals_bimap2, reverse=True), color=\"#2ecc71\"\n",
    ")\n",
    "ax7.set_title(\"After ReEig2\", fontsize=11, fontweight=\"bold\")\n",
    "ax7.set_yscale(\"log\")\n",
    "\n",
    "ax8 = axes[1, 3]\n",
    "logeig_vec = intermediates[\"logeig\"][0].detach().numpy()\n",
    "ax8.bar(range(len(logeig_vec)), logeig_vec, color=\"#9b59b6\")\n",
    "ax8.set_title(\"LogEig (tangent space)\", fontsize=11, fontweight=\"bold\")\n",
    "ax8.set_xlabel(\"Dimension\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Composing Custom Architectures\n\nThe modular design of SPD Learn allows flexible architecture composition.\nHere are some common patterns:\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class DeepSPDNet(nn.Module):\n",
    "    \"\"\"Deeper SPD network with multiple BiMap+ReEig blocks.\"\"\"\n",
    "\n",
    "    def __init__(self, n_channels, hidden_dims, n_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.cov = CovLayer()\n",
    "        self.shrinkage = Shrinkage(n_chans=n_channels)\n",
    "\n",
    "        # Build BiMap+ReEig blocks\n",
    "        self.blocks = nn.ModuleList()\n",
    "        dims = [n_channels] + list(hidden_dims)\n",
    "        for i in range(len(hidden_dims)):\n",
    "            self.blocks.append(\n",
    "                nn.Sequential(\n",
    "                    BiMap(in_features=dims[i], out_features=dims[i + 1]),\n",
    "                    ReEig(),\n",
    "                )\n",
    "            )\n",
    "\n",
    "        self.logeig = LogEig(upper=True)\n",
    "        final_dim = hidden_dims[-1] * (hidden_dims[-1] + 1) // 2\n",
    "        self.classifier = nn.Linear(final_dim, n_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cov(x)\n",
    "        x = self.shrinkage(x)\n",
    "\n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "\n",
    "        x = self.logeig(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "# Create a deeper network\n",
    "deep_model = DeepSPDNet(\n",
    "    n_channels=22,  # EEG with 22 channels\n",
    "    hidden_dims=[16, 8, 4],  # Progressive reduction\n",
    "    n_classes=4,  # 4-class motor imagery\n",
    ")\n",
    "\n",
    "# Count parameters\n",
    "n_params = sum(p.numel() for p in deep_model.parameters())\n",
    "print(\"\\nDeepSPDNet Architecture:\")\n",
    "print(\"  Input channels: 22\")\n",
    "print(\"  Hidden dimensions: 22 -> 16 -> 8 -> 4\")\n",
    "print(f\"  Output features: {4 * 5 // 2} (vectorized)\")\n",
    "print(f\"  Total parameters: {n_params:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary: The SPD Learning Pipeline\n\nThis tutorial covered the fundamental building blocks of SPD neural networks:\n\n1. **CovLayer**: Transforms raw signals to SPD covariance matrices\n\n2. **Shrinkage/TraceNorm**: Regularizes covariance matrices for stability\n\n3. **BiMap**: Performs geometry-preserving dimensionality reduction\n   using orthogonal projections\n\n4. **ReEig**: Introduces non-linearity by rectifying eigenvalues,\n   analogous to ReLU in standard networks\n\n5. **LogEig**: Maps SPD matrices to tangent space where Euclidean\n   operations apply, enabling standard linear classification\n\n**Key insights**:\n\n- All operations preserve SPD structure until LogEig\n- BiMap uses Stiefel manifold constraints for stable training\n  :cite:p:`brooks2019riemannian`\n- The pipeline gradually reduces dimensionality while preserving information\n- LogEig \"flattens\" the manifold for classification\n\nFor more advanced architectures, see the EEGSPDNet and TSMNet tutorials.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
