{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# SPD Learn Example\n",
    "# ==================\n",
    "#\n",
    "# First, install the required packages:\n",
    "\n",
    "!uv pip install -q spd_learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n\n# Cross-Session Transfer with TSMNet\n\nThis tutorial demonstrates how to use TSMNet for cross-session motor imagery\nclassification with domain adaptation. TSMNet's SPDBatchNorm layer enables\nadaptation to new sessions without labeled data from the target session.\n   :depth: 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n\nIn EEG-based BCIs, a common challenge is **session-to-session variability**:\nmodels trained on one day often perform poorly on another day\ndue to changes in electrode impedance, mental state, and environment.\n\nTSMNet :cite:p:`kobler2022spd` addresses this through **SPDBatchNorm**,\nwhich:\n\n1. Normalizes SPD matrices using the Fréchet mean\n2. Maintains running statistics that can be updated on new data\n3. Enables **Source-Free Unsupervised Domain Adaptation (SFUDA)**\n\nThis means we can adapt to a new subject using only unlabeled data!\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "from typing import List, Tuple\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from braindecode import EEGClassifier\n",
    "from moabb.datasets import BNCI2014_001\n",
    "from moabb.paradigms import MotorImagery\n",
    "from skada import (\n",
    "    CORALAdapter,\n",
    "    EntropicOTMapping,\n",
    "    SubspaceAlignment,\n",
    "    make_da_pipeline,\n",
    ")\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from skorch.callbacks import EpochScoring, GradientNormClipping\n",
    "from skorch.dataset import ValidSplit\n",
    "\n",
    "from spd_learn.models import TSMNet\n",
    "\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Dataset\n\nBNCI2014_001 contains EEG recordings from 9 subjects performing\n- **22 EEG channels**: Standard 10-20 montage\n- **250 Hz sampling rate**: After resampling\n\nWe'll demonstrate **cross-session transfer**:\n\n- **Source domain**: Subject 1, Session 1 (training)\n- **Target domain**: Subject 1, Session 2 (testing/adaptation)\n\nCross-session transfer is a realistic BCI scenario where we want to avoid\nrecalibration for a returning user.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset = BNCI2014_001()\n",
    "paradigm = MotorImagery(n_classes=4)\n",
    "\n",
    "print(f\"Dataset: {dataset.code}\")\n",
    "print(\"Cross-subject transfer: Subject 1 (source) -> Subject 2 (target)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the TSMNet Model\n\nTSMNet architecture:\n\n1. **Temporal Conv**: Learns temporal filters\n2. **Spatial Conv**: Learns spatial combinations\n3. **CovLayer**: Computes covariance matrices\n4. **BiMap + ReEig**: SPD dimensionality reduction\n5. **SPDBatchNorm**: Riemannian batch normalization (key for adaptation)\n6. **LogEig**: Projects to tangent space\n7. **Linear**: Classification head\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_chans = 22\n",
    "n_outputs = 4\n",
    "\n",
    "model = TSMNet(\n",
    "    n_chans=n_chans,\n",
    "    n_outputs=n_outputs,\n",
    "    n_temp_filters=8,  # Temporal filters\n",
    "    temp_kernel_length=50,  # ~200ms at 250Hz\n",
    "    n_spatiotemp_filters=32,  # Spatiotemporal features\n",
    "    n_bimap_filters=16,  # BiMap output dimension\n",
    "    reeig_threshold=1e-4,  # ReEig threshold\n",
    ")\n",
    "\n",
    "print(\"TSMNet Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on Source Domain\n\nFirst, we train TSMNet on Session 1 (source domain).\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "source_subject = 1\n",
    "target_subject = 1  # Same subject, different session\n",
    "batch_size = 32\n",
    "max_epochs = 300\n",
    "learning_rate = 1e-4  # Optimal learning rate from grid search\n",
    "weight_decay = 1e-4  # L2 regularization for better generalization\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"\\nUsing device: {device}\")\n",
    "\n",
    "# Cache configuration\n",
    "cache_config = dict(\n",
    "    save_raw=True,\n",
    "    save_epochs=True,\n",
    "    save_array=True,\n",
    "    use=True,\n",
    "    overwrite_raw=False,\n",
    "    overwrite_epochs=False,\n",
    "    overwrite_array=False,\n",
    ")\n",
    "\n",
    "# Load data for both subjects\n",
    "X, labels, meta = paradigm.get_data(\n",
    "    dataset=dataset,\n",
    "    subjects=[source_subject, target_subject],\n",
    "    cache_config=cache_config,\n",
    ")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(labels)\n",
    "\n",
    "# Split by session\n",
    "# Session '0train' is the first session (source)\n",
    "# Session '1test' is the second session (target)\n",
    "source_idx = meta.query(\"session == '0train'\").index.to_numpy()\n",
    "target_idx = meta.query(\"session == '1test'\").index.to_numpy()\n",
    "\n",
    "X_source, y_source = X[source_idx], y[source_idx]\n",
    "X_target, y_target = X[target_idx], y[target_idx]\n",
    "\n",
    "print(f\"\\nSource domain (Session 1): {len(source_idx)} samples\")\n",
    "print(f\"Target domain (Session 2): {len(target_idx)} samples\")\n",
    "\n",
    "# Create classifier\n",
    "# Note: SPD networks benefit from gradient clipping to prevent\n",
    "# divergence during training on the Riemannian manifold.\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.CrossEntropyLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    optimizer__lr=learning_rate,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    train_split=ValidSplit(0.1, stratified=True, random_state=42),\n",
    "    batch_size=batch_size,\n",
    "    max_epochs=max_epochs,\n",
    "    callbacks=[\n",
    "        (\n",
    "            \"train_acc\",\n",
    "            EpochScoring(\n",
    "                \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n",
    "            ),\n",
    "        ),\n",
    "        (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n",
    "    ],\n",
    "    device=device,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "# Train on source domain\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Training on Source Domain\")\n",
    "print(\"=\" * 50)\n",
    "clf.fit(X_source, y_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Without Adaptation\n\nLet's first see how the model performs on the target domain\nWITHOUT any adaptation.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate on source (should be high)\n",
    "y_pred_source = clf.predict(X_source)\n",
    "source_acc = accuracy_score(y_source, y_pred_source)\n",
    "\n",
    "# Evaluate on target WITHOUT adaptation\n",
    "y_pred_target_no_adapt = clf.predict(X_target)\n",
    "target_acc_no_adapt = accuracy_score(y_target, y_pred_target_no_adapt)\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Results WITHOUT Domain Adaptation\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Source Domain Accuracy: {source_acc*100:.2f}%\")\n",
    "print(f\"Target Domain Accuracy: {target_acc_no_adapt*100:.2f}%\")\n",
    "print(f\"Performance Drop: {(source_acc - target_acc_no_adapt)*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation via SPDBatchNorm\n\nNow we perform **Source-Free Unsupervised Domain Adaptation (SFUDA)**:\n\n1. Put the model in eval mode (freeze all parameters)\n2. Put SPDBatchNorm in train mode (update running statistics)\n3. Pass target domain data through the model (no labels needed!)\n4. The running mean adapts to the target domain distribution\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def adapt_spdbn(\n",
    "    model,\n",
    "    X_target,\n",
    "    n_passes=10,\n",
    "    reset_stats=False,\n",
    "    adapt_momentum=0.8,\n",
    "    batch_size=64,\n",
    "):\n",
    "    \"\"\"Adapt SPDBatchNorm statistics to target domain.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        TSMNet model with SPDBatchNorm layer.\n",
    "    X_target : array\n",
    "        Target domain data (unlabeled).\n",
    "    n_passes : int\n",
    "        Number of passes through the data for statistics update.\n",
    "    reset_stats : bool\n",
    "        If True, reset running statistics before adaptation.\n",
    "        Default is False to preserve source domain knowledge.\n",
    "    adapt_momentum : float\n",
    "        Momentum to use during adaptation (higher = faster adaptation).\n",
    "    batch_size : int\n",
    "        Batch size for adaptation (larger = more stable statistics).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    model : nn.Module\n",
    "        The adapted model with updated SPDBatchNorm statistics.\n",
    "    \"\"\"\n",
    "    model.eval()  # Freeze other layers\n",
    "\n",
    "    # Find SPDBatchNorm layers and configure for adaptation\n",
    "    spdbn_modules = []\n",
    "    original_momentums = []\n",
    "    for module in model.modules():\n",
    "        class_name = module.__class__.__name__\n",
    "        if \"SPDBatchNorm\" in class_name:\n",
    "            spdbn_modules.append(module)\n",
    "            original_momentums.append(module.momentum)\n",
    "\n",
    "            if reset_stats:\n",
    "                # Reset to identity mean and unit variance for fresh adaptation\n",
    "                module.reset_running_stats()\n",
    "\n",
    "            module.train()  # Enable running stats update\n",
    "\n",
    "    print(f\"Found {len(spdbn_modules)} SPDBatchNorm layer(s) to adapt\")\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.tensor(X_target, dtype=torch.float32)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        X_tensor = X_tensor.cuda()\n",
    "\n",
    "    # Pass data through model multiple times to update statistics\n",
    "    with torch.no_grad():\n",
    "        for pass_idx in range(n_passes):\n",
    "            # Set momentum for this pass\n",
    "            for module in spdbn_modules:\n",
    "                module.momentum = adapt_momentum\n",
    "\n",
    "            # Reshuffle each pass for better statistics\n",
    "            perm = torch.randperm(len(X_tensor))\n",
    "            X_shuffled = X_tensor[perm]\n",
    "\n",
    "            # Process in batches\n",
    "            for i in range(0, len(X_shuffled), batch_size):\n",
    "                batch = X_shuffled[i : i + batch_size]\n",
    "                _ = model(batch)\n",
    "\n",
    "            if (pass_idx + 1) % 10 == 0 or pass_idx == 0:\n",
    "                print(f\"  Adaptation pass {pass_idx + 1}/{n_passes}\")\n",
    "\n",
    "    # Restore original momentum values\n",
    "    for module, orig_momentum in zip(spdbn_modules, original_momentums):\n",
    "        module.momentum = orig_momentum\n",
    "\n",
    "    model.eval()  # Set everything back to eval\n",
    "    return model\n",
    "\n",
    "\n",
    "def predict_with_domain_specific_bn(model, X_data):\n",
    "    \"\"\"Predict using domain-specific batch normalization (SPDDSMBN approach).\n",
    "\n",
    "    This implements the key idea from Kobler et al. (NeurIPS 2022):\n",
    "    Compute domain-specific statistics on the target domain and use them\n",
    "    directly for normalization. This is different from standard adaptation\n",
    "    which tries to blend source and target statistics.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        TSMNet model with SPDBatchNorm layer.\n",
    "    X_data : array\n",
    "        Target domain data to predict on.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    predictions : array\n",
    "        Predicted class labels.\n",
    "    \"\"\"\n",
    "    # Find SPDBatchNorm layers\n",
    "    spdbn_modules = []\n",
    "    for module in model.modules():\n",
    "        class_name = module.__class__.__name__\n",
    "        if \"SPDBatchNorm\" in class_name:\n",
    "            spdbn_modules.append(module)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Convert to tensor\n",
    "    X_tensor = torch.tensor(X_data, dtype=torch.float32)\n",
    "    if next(model.parameters()).is_cuda:\n",
    "        X_tensor = X_tensor.cuda()\n",
    "\n",
    "    # Key insight from SPDDSMBN: Compute domain-specific statistics\n",
    "    # by processing ALL target data with momentum=1.0 (full batch stats)\n",
    "    # This estimates the target domain's Fréchet mean and variance\n",
    "    for module in spdbn_modules:\n",
    "        module.reset_running_stats()  # Start fresh for target domain\n",
    "        module.momentum = 1.0  # Use full batch statistics\n",
    "        module.train()  # Enable running stats update\n",
    "\n",
    "    # Single pass to compute target domain statistics using all data\n",
    "    with torch.no_grad():\n",
    "        _ = model(X_tensor)  # This updates running_mean and running_var\n",
    "\n",
    "    # Now predict using the target domain statistics\n",
    "    model.eval()  # Back to eval mode - uses the updated running stats\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(X_tensor)\n",
    "        predictions = logits.argmax(dim=1).cpu().numpy()\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def extract_features_from_tsmnet(model, X, batch_size=32):\n",
    "    \"\"\"Extract tangent space features from TSMNet (before classification head).\n",
    "\n",
    "    This extracts the Euclidean features from the tangent space projection,\n",
    "    which can be used with standard domain adaptation methods like CORAL,\n",
    "    Subspace Alignment, and Optimal Transport.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : nn.Module\n",
    "        TSMNet model.\n",
    "    X : array\n",
    "        Input EEG data of shape (n_samples, n_channels, n_times).\n",
    "    batch_size : int\n",
    "        Batch size for processing.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    features : np.ndarray\n",
    "        Tangent space features of shape (n_samples, n_features).\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
    "    device = next(model.parameters()).device\n",
    "    X_tensor = X_tensor.to(device)\n",
    "\n",
    "    features_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(X_tensor), batch_size):\n",
    "            batch = X_tensor[i : i + batch_size]\n",
    "            # Process through TSMNet layers up to LogEig (before classification head)\n",
    "            x = batch[:, None, ...]  # Add channel dim for CNN\n",
    "            x = model.cnn(x)\n",
    "            x = model.covpool(x)\n",
    "            x = model.spdnet(x)\n",
    "            x = model.spdbnorm(x)\n",
    "            x = model.logeig(x)  # Tangent space features\n",
    "            features_list.append(x.cpu())\n",
    "\n",
    "    return torch.cat(features_list, dim=0).numpy()\n",
    "\n",
    "\n",
    "def plot_domain_shift_comprehensive(\n",
    "    features_source: np.ndarray,\n",
    "    features_target: np.ndarray,\n",
    "    y_source: np.ndarray,\n",
    "    y_target: np.ndarray,\n",
    "    class_names: List[str],\n",
    "    title: str = \"Domain Shift Visualization\",\n",
    "    figsize: Tuple[int, int] = (16, 12),\n",
    ") -> plt.Figure:\n",
    "    \"\"\"Comprehensive visualization of domain shift.\n",
    "\n",
    "    Creates a 2x3 grid showing:\n",
    "    - Domain distributions (PCA)\n",
    "    - Source and target by class\n",
    "    - Feature histograms per domain\n",
    "    - Class-conditional distributions\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features_source : np.ndarray\n",
    "        Source domain features.\n",
    "    features_target : np.ndarray\n",
    "        Target domain features.\n",
    "    y_source : np.ndarray\n",
    "        Source domain labels.\n",
    "    y_target : np.ndarray\n",
    "        Target domain labels.\n",
    "    class_names : List[str]\n",
    "        Names of the classes.\n",
    "    title : str\n",
    "        Overall figure title.\n",
    "    figsize : Tuple[int, int]\n",
    "        Figure size.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    plt.Figure\n",
    "        The matplotlib figure.\n",
    "    \"\"\"\n",
    "    # Combine features for PCA\n",
    "    features_all = np.vstack([features_source, features_target])\n",
    "    pca = PCA(n_components=2)\n",
    "    features_2d = pca.fit_transform(features_all)\n",
    "\n",
    "    n_source = len(features_source)\n",
    "    source_2d = features_2d[:n_source]\n",
    "    target_2d = features_2d[n_source:]\n",
    "\n",
    "    fig, axes = plt.subplots(2, 3, figsize=figsize)\n",
    "\n",
    "    # --- Row 1 ---\n",
    "    # Plot 1: All data by domain\n",
    "    ax1 = axes[0, 0]\n",
    "    ax1.scatter(\n",
    "        source_2d[:, 0],\n",
    "        source_2d[:, 1],\n",
    "        c=\"blue\",\n",
    "        alpha=0.5,\n",
    "        label=\"Source\",\n",
    "        marker=\"o\",\n",
    "        s=40,\n",
    "    )\n",
    "    ax1.scatter(\n",
    "        target_2d[:, 0],\n",
    "        target_2d[:, 1],\n",
    "        c=\"red\",\n",
    "        alpha=0.5,\n",
    "        label=\"Target\",\n",
    "        marker=\"s\",\n",
    "        s=40,\n",
    "    )\n",
    "    ax1.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    ax1.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "    ax1.set_title(\"Domain Distribution\", fontweight=\"bold\")\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 2: Source domain by class\n",
    "    ax2 = axes[0, 1]\n",
    "    n_classes = len(np.unique(y_source))\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, max(n_classes, 4)))[:n_classes]\n",
    "    for label_idx, label in enumerate(np.unique(y_source)):\n",
    "        mask = y_source == label\n",
    "        ax2.scatter(\n",
    "            source_2d[mask, 0],\n",
    "            source_2d[mask, 1],\n",
    "            c=colors[label_idx],\n",
    "            alpha=0.6,\n",
    "            label=f\"{class_names[label_idx]}\",\n",
    "            marker=\"o\",\n",
    "            s=40,\n",
    "        )\n",
    "    ax2.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    ax2.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "    ax2.set_title(\"Source Domain (by class)\", fontweight=\"bold\")\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 3: Target domain by class\n",
    "    ax3 = axes[0, 2]\n",
    "    for label_idx, label in enumerate(np.unique(y_target)):\n",
    "        mask = y_target == label\n",
    "        ax3.scatter(\n",
    "            target_2d[mask, 0],\n",
    "            target_2d[mask, 1],\n",
    "            c=colors[label_idx],\n",
    "            alpha=0.6,\n",
    "            label=f\"{class_names[label_idx]}\",\n",
    "            marker=\"s\",\n",
    "            s=40,\n",
    "        )\n",
    "    ax3.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\")\n",
    "    ax3.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\")\n",
    "    ax3.set_title(\"Target Domain (by class)\", fontweight=\"bold\")\n",
    "    ax3.legend()\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # --- Row 2 ---\n",
    "    # Plot 4: Feature histogram (first 3 features)\n",
    "    ax4 = axes[1, 0]\n",
    "    for feat_idx in range(min(3, features_source.shape[1])):\n",
    "        ax4.hist(\n",
    "            features_source[:, feat_idx],\n",
    "            bins=20,\n",
    "            alpha=0.5,\n",
    "            label=f\"Source feat {feat_idx}\",\n",
    "            density=True,\n",
    "        )\n",
    "        ax4.hist(\n",
    "            features_target[:, feat_idx],\n",
    "            bins=20,\n",
    "            alpha=0.5,\n",
    "            label=f\"Target feat {feat_idx}\",\n",
    "            density=True,\n",
    "            linestyle=\"--\",\n",
    "        )\n",
    "    ax4.set_xlabel(\"Feature Value\")\n",
    "    ax4.set_ylabel(\"Density\")\n",
    "    ax4.set_title(\"Feature Distribution (first 3)\", fontweight=\"bold\")\n",
    "    ax4.legend(fontsize=8)\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 5: Class-conditional distributions (Source)\n",
    "    ax5 = axes[1, 1]\n",
    "    for label_idx, label in enumerate(np.unique(y_source)):\n",
    "        mask_s = y_source == label\n",
    "        ax5.hist(\n",
    "            source_2d[mask_s, 0],\n",
    "            bins=15,\n",
    "            alpha=0.5,\n",
    "            label=f\"Source-{class_names[label_idx]}\",\n",
    "            color=colors[label_idx],\n",
    "            density=True,\n",
    "        )\n",
    "    ax5.set_xlabel(\"PC1\")\n",
    "    ax5.set_ylabel(\"Density\")\n",
    "    ax5.set_title(\"Class Distribution (Source)\", fontweight=\"bold\")\n",
    "    ax5.legend()\n",
    "    ax5.grid(True, alpha=0.3)\n",
    "\n",
    "    # Plot 6: Class-conditional distributions (Target)\n",
    "    ax6 = axes[1, 2]\n",
    "    for label_idx, label in enumerate(np.unique(y_target)):\n",
    "        mask_t = y_target == label\n",
    "        ax6.hist(\n",
    "            target_2d[mask_t, 0],\n",
    "            bins=15,\n",
    "            alpha=0.5,\n",
    "            label=f\"Target-{class_names[label_idx]}\",\n",
    "            color=colors[label_idx],\n",
    "            density=True,\n",
    "        )\n",
    "    ax6.set_xlabel(\"PC1\")\n",
    "    ax6.set_ylabel(\"Density\")\n",
    "    ax6.set_title(\"Class Distribution (Target)\", fontweight=\"bold\")\n",
    "    ax6.legend()\n",
    "    ax6.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(title, fontsize=14, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    return fig\n",
    "\n",
    "\n",
    "# Get the underlying model from the classifier\n",
    "underlying_model = clf.module_\n",
    "\n",
    "# Use Domain-Specific Batch Normalization (SPDDSMBN approach from Kobler et al.)\n",
    "# This computes target-domain-specific statistics and uses them for normalization\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Using Domain-Specific Batch Normalization (SPDDSMBN)\")\n",
    "print(\"=\" * 50)\n",
    "print(\"Computing target domain statistics (Fréchet mean and variance)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating After Adaptation\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Cross-session transfer typically shows distribution shifts\n   that SPDBatchNorm can correct.\n   The improvement depends on:\n\n   - Non-stationarity between sessions\n   - Training convergence on the source session\n   - How well the learned features generalize\n\n   Typical improvements range from 3-10% for cross-session transfer.</p></div>\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Predict using domain-specific batch normalization\n",
    "# Key insight: Use target domain statistics directly (not blended with source)\n",
    "y_pred_target_adapted = predict_with_domain_specific_bn(underlying_model, X_target)\n",
    "\n",
    "target_acc_adapted = accuracy_score(y_target, y_pred_target_adapted)\n",
    "improvement = target_acc_adapted - target_acc_no_adapt\n",
    "\n",
    "print(f\"\\n{'='*50}\")\n",
    "print(\"Results WITH Domain Adaptation\")\n",
    "print(f\"{'='*50}\")\n",
    "print(f\"Target Accuracy (No Adaptation):   {target_acc_no_adapt*100:.2f}%\")\n",
    "print(f\"Target Accuracy (With Adaptation): {target_acc_adapted*100:.2f}%\")\n",
    "if improvement >= 0:\n",
    "    print(f\"Improvement: +{improvement*100:.2f}%\")\n",
    "else:\n",
    "    print(f\"Improvement: {improvement*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Domain Adaptation with SKADA\n\nNow we compare SPDBatchNorm/TTBN with domain adaptation methods from\n[skada](https://scikit-adaptation.github.io/) (scikit-learn domain\nadaptation). These methods operate on the Euclidean tangent space\nfeatures extracted from TSMNet.\n\n**Methods compared:**\n\n- **CORAL**: Correlation Alignment - aligns second-order statistics\n- **Subspace Alignment**: Linear subspace mapping between domains\n- **Entropic Optimal Transport**: Sample-to-sample mapping\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SKADA Domain Adaptation Methods\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Extract tangent space features for SKADA methods\n",
    "features_source = extract_features_from_tsmnet(underlying_model, X_source)\n",
    "features_target = extract_features_from_tsmnet(underlying_model, X_target)\n",
    "\n",
    "print(f\"Source features shape: {features_source.shape}\")\n",
    "print(f\"Target features shape: {features_target.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Domain Shift\n\nBefore applying domain adaptation, let's visualize the distribution\nshift between source and target sessions using PCA projection.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class_names = [str(c) for c in le.classes_]\n",
    "fig = plot_domain_shift_comprehensive(\n",
    "    features_source,\n",
    "    features_target,\n",
    "    y_source,\n",
    "    y_target,\n",
    "    class_names=class_names,\n",
    "    title=\"Riemannian Feature Space - Cross-Session Distribution\",\n",
    ")\n",
    "plt.show()\n",
    "\n",
    "# Prepare data in SKADA format\n",
    "# SKADA uses sample_domain to distinguish domains:\n",
    "# - Positive values (1): Source domain\n",
    "# - Negative values (-1): Target domain\n",
    "X_combined = np.vstack([features_source, features_target])\n",
    "y_combined = np.concatenate([y_source, -np.ones(len(y_target))])\n",
    "sample_domain = np.concatenate(\n",
    "    [np.ones(len(features_source)), -np.ones(len(features_target))]\n",
    ")\n",
    "\n",
    "# Initialize results dictionary\n",
    "results = {\n",
    "    \"No Adaptation\": target_acc_no_adapt,\n",
    "    \"SPDBatchNorm (TTBN)\": target_acc_adapted,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CORAL (Correlation Alignment)\n\nCORAL aligns the second-order statistics (covariance) of source and\ntarget feature distributions. This is particularly suitable for\nSPD-derived features since they already capture covariance structure.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"CORAL (Correlation Alignment)\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "coral_pipeline = make_da_pipeline(\n",
    "    StandardScaler(),\n",
    "    CORALAdapter(reg=1e-3),\n",
    "    LogisticRegression(max_iter=1000),\n",
    ")\n",
    "coral_pipeline.fit(X_combined, y_combined, sample_domain=sample_domain)\n",
    "y_pred_coral = coral_pipeline.predict(features_target)\n",
    "coral_acc = accuracy_score(y_target, y_pred_coral)\n",
    "\n",
    "print(f\"CORAL Accuracy: {coral_acc*100:.2f}%\")\n",
    "print(f\"Improvement over baseline: {(coral_acc - target_acc_no_adapt)*100:+.2f}%\")\n",
    "results[\"CORAL\"] = coral_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subspace Alignment\n\nSubspace Alignment learns a linear transformation that aligns the\nprincipal subspaces of source and target domains.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Subspace Alignment\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "sa_clf = SubspaceAlignment(\n",
    "    base_estimator=LogisticRegression(max_iter=1000),\n",
    "    n_components=min(10, features_source.shape[1]),\n",
    ")\n",
    "sa_clf.fit(X_combined, y_combined, sample_domain=sample_domain)\n",
    "y_pred_sa = sa_clf.predict(features_target)\n",
    "sa_acc = accuracy_score(y_target, y_pred_sa)\n",
    "\n",
    "print(f\"Subspace Alignment Accuracy: {sa_acc*100:.2f}%\")\n",
    "print(f\"Improvement over baseline: {(sa_acc - target_acc_no_adapt)*100:+.2f}%\")\n",
    "results[\"Subspace Alignment\"] = sa_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entropic Optimal Transport\n\nOptimal Transport finds the minimum cost mapping between source and\ntarget distributions. Entropic regularization makes the optimization\ntractable and provides smoother mappings.\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"-\" * 50)\n",
    "print(\"Entropic Optimal Transport\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    ot_clf = EntropicOTMapping(\n",
    "        base_estimator=LogisticRegression(max_iter=1000),\n",
    "        reg_e=1.0,\n",
    "    )\n",
    "    ot_clf.fit(X_combined, y_combined, sample_domain=sample_domain)\n",
    "    y_pred_ot = ot_clf.predict(features_target)\n",
    "    ot_acc = accuracy_score(y_target, y_pred_ot)\n",
    "\n",
    "    print(f\"Entropic OT Accuracy: {ot_acc*100:.2f}%\")\n",
    "    print(f\"Improvement over baseline: {(ot_acc - target_acc_no_adapt)*100:+.2f}%\")\n",
    "    results[\"Entropic OT\"] = ot_acc\n",
    "except Exception as e:\n",
    "    print(f\"Entropic OT failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Summary\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Domain Adaptation Results Summary\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':<25} {'Accuracy':>12} {'vs Baseline':>14}\")\n",
    "print(\"-\" * 55)\n",
    "for method, acc in results.items():\n",
    "    if method == \"No Adaptation\":\n",
    "        print(f\"{method:<25} {acc*100:>10.2f}% {'-':>14}\")\n",
    "    else:\n",
    "        imp = acc - target_acc_no_adapt\n",
    "        print(f\"{method:<25} {acc*100:>10.2f}% {imp*100:>+12.2f}%\")\n",
    "print(\"-\" * 55)\n",
    "print(\"Chance level: 25.00% (4 classes)\")\n",
    "\n",
    "# Find best method\n",
    "best_method = max(results.keys(), key=lambda k: results[k])\n",
    "print(f\"\\nBest method: {best_method} ({results[best_method]*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Results\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
    "\n",
    "# 1. Domain Adaptation Methods Comparison\n",
    "ax1 = axes[0]\n",
    "methods = list(results.keys())\n",
    "accuracies = [results[m] * 100 for m in methods]\n",
    "colors = [\"#e74c3c\", \"#3498db\", \"#2ecc71\", \"#9b59b6\", \"#f39c12\"][: len(methods)]\n",
    "bars = ax1.bar(methods, accuracies, color=colors, edgecolor=\"black\", linewidth=1.5)\n",
    "ax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\n",
    "ax1.set_title(\"Domain Adaptation Comparison\", fontsize=14)\n",
    "ax1.set_ylim([0, 100])\n",
    "ax1.axhline(y=25, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Chance (25%)\")\n",
    "ax1.axhline(\n",
    "    y=source_acc * 100,\n",
    "    color=\"blue\",\n",
    "    linestyle=\":\",\n",
    "    alpha=0.5,\n",
    "    label=f\"Source ({source_acc*100:.1f}%)\",\n",
    ")\n",
    "\n",
    "# Add value labels\n",
    "for bar, acc in zip(bars, accuracies):\n",
    "    ax1.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        bar.get_height() + 2,\n",
    "        f\"{acc:.1f}%\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=9,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax1.legend(loc=\"lower right\", fontsize=8)\n",
    "plt.setp(ax1.xaxis.get_majorticklabels(), rotation=30, ha=\"right\")\n",
    "\n",
    "# 2. Training history\n",
    "ax2 = axes[1]\n",
    "history = clf.history\n",
    "epochs_hist = range(1, len(history) + 1)\n",
    "ax2.plot(epochs_hist, history[:, \"train_loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\n",
    "ax2.plot(epochs_hist, history[:, \"valid_loss\"], \"r--\", label=\"Valid Loss\", linewidth=2)\n",
    "ax2.set_xlabel(\"Epoch\", fontsize=12)\n",
    "ax2.set_ylabel(\"Loss\", fontsize=12)\n",
    "ax2.set_title(\"Training History\", fontsize=14)\n",
    "ax2.legend(fontsize=10)\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Feature space PCA visualization\n",
    "ax3 = axes[2]\n",
    "pca = PCA(n_components=2)\n",
    "features_all = np.vstack([features_source, features_target])\n",
    "features_2d = pca.fit_transform(features_all)\n",
    "n_source = len(features_source)\n",
    "\n",
    "ax3.scatter(\n",
    "    features_2d[:n_source, 0],\n",
    "    features_2d[:n_source, 1],\n",
    "    c=\"blue\",\n",
    "    alpha=0.5,\n",
    "    label=\"Source\",\n",
    "    marker=\"o\",\n",
    "    s=30,\n",
    ")\n",
    "ax3.scatter(\n",
    "    features_2d[n_source:, 0],\n",
    "    features_2d[n_source:, 1],\n",
    "    c=\"red\",\n",
    "    alpha=0.5,\n",
    "    label=\"Target\",\n",
    "    marker=\"s\",\n",
    "    s=30,\n",
    ")\n",
    "ax3.set_xlabel(f\"PC1 ({pca.explained_variance_ratio_[0]*100:.1f}%)\", fontsize=12)\n",
    "ax3.set_ylabel(f\"PC2 ({pca.explained_variance_ratio_[1]*100:.1f}%)\", fontsize=12)\n",
    "ax3.set_title(\"Feature Space (PCA)\", fontsize=14)\n",
    "ax3.legend(fontsize=10)\n",
    "ax3.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding SPDBatchNorm Adaptation\n\nThe key insight is that **session variability manifests as a shift in\nthe distribution of SPD matrices**. SPDBatchNorm counters this by:\n\n1. **Centering**: Removes the batch mean (Fréchet mean on SPD manifold)\n\n   .. math::\n\n      \\tilde{P}_i = G^{-1/2} P_i G^{-1/2}\n\n2. **Scaling**: Normalizes dispersion\n\n   .. math::\n\n      \\hat{P}_i = \\tilde{P}_i^{w/\\sqrt{\\sigma^2 + \\varepsilon}}\n\nWhen we adapt, we update the running mean $G$ and variance\n$\\sigma^2$ to match the target domain, aligning the distributions\nwithout any labeled data.\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n\nIn this tutorial, we demonstrated:\n\n1. Training TSMNet on source session\n2. Observing performance drop on target session\n3. Adapting using SPDBatchNorm (Test-Time Batch Normalization)\n4. Comparing with SKADA domain adaptation methods:\n\n   - **CORAL**: Correlation Alignment\n   - **Subspace Alignment**: Linear subspace mapping\n   - **Entropic Optimal Transport**: Sample-to-sample mapping\n\n**Key insights:**\n\n- SPDBatchNorm provides a native Riemannian approach that operates\n  directly on SPD matrices\n- SKADA methods operate on Euclidean tangent space features and can\n  complement or outperform SPDBatchNorm depending on the domain shift\n- Combining multiple approaches allows practitioners to select the best\n  method for their specific use case\n\nCross-session non-stationarity is a key challenge in BCI. Domain\nadaptation methods compensate for these shifts by aligning feature\ndistributions between source and target domains.\n\nThis **unsupervised domain adaptation** is particularly valuable in BCI\napplications where:\n\n- Calibration time should be minimized\n- Users return for multiple sessions\n- Signal properties drift over time\n\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "plt.close(\"all\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
