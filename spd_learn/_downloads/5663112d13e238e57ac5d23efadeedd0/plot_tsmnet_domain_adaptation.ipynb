{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# SPD Learn Example\n# ==================\n#\n# First, install the required packages:\n\n!uv pip install -q spd_learn"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Cross-Session Transfer with TSMNet\n\nThis tutorial demonstrates how to use TSMNet for cross-session motor imagery\nclassification with domain adaptation. TSMNet's SPDBatchNorm layer enables\nadaptation to new sessions without labeled data from the target session.\n   :depth: 2\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Introduction\n\nIn EEG-based BCIs, a common challenge is **session-to-session variability**:\nmodels trained on one day often perform poorly on another day\ndue to changes in electrode impedance, mental state, and environment.\n\nTSMNet :cite:p:`kobler2022spd` addresses this through **SPDBatchNorm**,\nwhich:\n\n1. Normalizes SPD matrices using the Fr\u00e9chet mean\n2. Maintains running statistics that can be updated on new data\n3. Enables **Source-Free Unsupervised Domain Adaptation (SFUDA)**\n\nThis means we can adapt to a new subject using only unlabeled data!\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup and Imports\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\n\nimport matplotlib.pyplot as plt\nimport torch\n\nfrom braindecode import EEGClassifier\nfrom moabb.datasets import BNCI2014_001\nfrom moabb.paradigms import MotorImagery\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom skorch.callbacks import EpochScoring, GradientNormClipping\nfrom skorch.dataset import ValidSplit\n\nfrom spd_learn.models import TSMNet\n\n\nwarnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the Dataset\n\nBNCI2014_001 contains EEG recordings from 9 subjects performing\n- **22 EEG channels**: Standard 10-20 montage\n- **250 Hz sampling rate**: After resampling\n\nWe'll demonstrate **cross-session transfer**:\n\n- **Source domain**: Subject 1, Session 1 (training)\n- **Target domain**: Subject 1, Session 2 (testing/adaptation)\n\nCross-session transfer is a realistic BCI scenario where we want to avoid\nrecalibration for a returning user.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dataset = BNCI2014_001()\nparadigm = MotorImagery(n_classes=4)\n\nprint(f\"Dataset: {dataset.code}\")\nprint(\"Cross-subject transfer: Subject 1 (source) -> Subject 2 (target)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating the TSMNet Model\n\nTSMNet architecture:\n\n1. **Temporal Conv**: Learns temporal filters\n2. **Spatial Conv**: Learns spatial combinations\n3. **CovLayer**: Computes covariance matrices\n4. **BiMap + ReEig**: SPD dimensionality reduction\n5. **SPDBatchNorm**: Riemannian batch normalization (key for adaptation)\n6. **LogEig**: Projects to tangent space\n7. **Linear**: Classification head\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "n_chans = 22\nn_outputs = 4\n\nmodel = TSMNet(\n    n_chans=n_chans,\n    n_outputs=n_outputs,\n    n_temp_filters=8,  # Temporal filters (increased)\n    temp_kernel_length=50,  # ~200ms at 250Hz\n    n_spatiotemp_filters=32,  # Spatiotemporal features\n    n_bimap_filters=16,  # BiMap output dimension\n    reeig_threshold=1e-4,  # ReEig threshold\n)\n\nprint(\"TSMNet Architecture:\")\nprint(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Training on Source Domain\n\nFirst, we train TSMNet on Session 1 (source domain).\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "source_subject = 1\ntarget_subject = 1  # Same subject, different session\nbatch_size = 32\nmax_epochs = 100\nlearning_rate = 1e-4  # Low learning rate for stable SPD learning\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"\\nUsing device: {device}\")\n\n# Cache configuration\ncache_config = dict(\n    save_raw=True,\n    save_epochs=True,\n    save_array=True,\n    use=True,\n    overwrite_raw=False,\n    overwrite_epochs=False,\n    overwrite_array=False,\n)\n\n# Load data for both subjects\nX, labels, meta = paradigm.get_data(\n    dataset=dataset,\n    subjects=[source_subject, target_subject],\n    cache_config=cache_config,\n)\n\n# Encode labels\nle = LabelEncoder()\ny = le.fit_transform(labels)\n\n# Split by session\n# Session '0train' is the first session (source)\n# Session '1test' is the second session (target)\nsource_idx = meta.query(\"session == '0train'\").index.to_numpy()\ntarget_idx = meta.query(\"session == '1test'\").index.to_numpy()\n\nX_source, y_source = X[source_idx], y[source_idx]\nX_target, y_target = X[target_idx], y[target_idx]\n\nprint(f\"\\nSource domain (Session 1): {len(source_idx)} samples\")\nprint(f\"Target domain (Session 2): {len(target_idx)} samples\")\n\n# Create classifier\n# Note: SPD networks benefit from gradient clipping to prevent\n# divergence during training on the Riemannian manifold.\nclf = EEGClassifier(\n    model,\n    criterion=torch.nn.CrossEntropyLoss,\n    optimizer=torch.optim.Adam,\n    optimizer__lr=learning_rate,\n    train_split=ValidSplit(0.1, stratified=True, random_state=42),\n    batch_size=batch_size,\n    max_epochs=max_epochs,\n    callbacks=[\n        (\n            \"train_acc\",\n            EpochScoring(\n                \"accuracy\", lower_is_better=False, on_train=True, name=\"train_acc\"\n            ),\n        ),\n        (\"gradient_clip\", GradientNormClipping(gradient_clip_value=1.0)),\n    ],\n    device=device,\n    verbose=1,\n)\n\n# Train on source domain\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Training on Source Domain\")\nprint(\"=\" * 50)\nclf.fit(X_source, y_source)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating Without Adaptation\n\nLet's first see how the model performs on the target domain\nWITHOUT any adaptation.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Evaluate on source (should be high)\ny_pred_source = clf.predict(X_source)\nsource_acc = accuracy_score(y_source, y_pred_source)\n\n# Evaluate on target WITHOUT adaptation\ny_pred_target_no_adapt = clf.predict(X_target)\ntarget_acc_no_adapt = accuracy_score(y_target, y_pred_target_no_adapt)\n\nprint(f\"\\n{'='*50}\")\nprint(\"Results WITHOUT Domain Adaptation\")\nprint(f\"{'='*50}\")\nprint(f\"Source Domain Accuracy: {source_acc*100:.2f}%\")\nprint(f\"Target Domain Accuracy: {target_acc_no_adapt*100:.2f}%\")\nprint(f\"Performance Drop: {(source_acc - target_acc_no_adapt)*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Domain Adaptation via SPDBatchNorm\n\nNow we perform **Source-Free Unsupervised Domain Adaptation (SFUDA)**:\n\n1. Put the model in eval mode (freeze all parameters)\n2. Put SPDBatchNorm in train mode (update running statistics)\n3. Pass target domain data through the model (no labels needed!)\n4. The running mean adapts to the target domain distribution\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "def adapt_spdbn(model, X_target, n_passes=3, reset_stats=True):\n    \"\"\"Adapt SPDBatchNorm statistics to target domain.\n\n    Parameters\n    ----------\n    model : nn.Module\n        TSMNet model with SPDBatchNorm layer.\n    X_target : array\n        Target domain data (unlabeled).\n    n_passes : int\n        Number of passes through the data for statistics update.\n    reset_stats : bool\n        If True, reset running statistics before adaptation.\n        This allows the model to fully adapt to the target domain.\n\n    Returns\n    -------\n    model : nn.Module\n        The adapted model with updated SPDBatchNorm statistics.\n    \"\"\"\n    model.eval()  # Freeze other layers\n\n    # Find SPDBatchNorm layers (may be wrapped as ParametrizedSPDBatchNorm)\n    spdbn_modules = []\n    for module in model.modules():\n        class_name = module.__class__.__name__\n        if \"SPDBatchNorm\" in class_name:\n            spdbn_modules.append(module)\n            if reset_stats:\n                # Reset to identity mean and unit variance for fresh adaptation\n                module.reset_running_stats()\n            module.train()  # Enable running stats update\n\n    print(f\"Found {len(spdbn_modules)} SPDBatchNorm layer(s) to adapt\")\n\n    # Convert to tensor\n    X_tensor = torch.tensor(X_target, dtype=torch.float32)\n    if next(model.parameters()).is_cuda:\n        X_tensor = X_tensor.cuda()\n\n    # Shuffle data for better statistics estimation\n    perm = torch.randperm(len(X_tensor))\n    X_tensor = X_tensor[perm]\n\n    # Pass data through model multiple times to update statistics\n    with torch.no_grad():\n        for pass_idx in range(n_passes):\n            # Process in batches\n            for i in range(0, len(X_tensor), 32):\n                batch = X_tensor[i : i + 32]\n                _ = model(batch)\n            print(f\"  Adaptation pass {pass_idx + 1}/{n_passes} complete\")\n\n    model.eval()  # Set everything back to eval\n    return model\n\n\n# Get the underlying model from the classifier\nunderlying_model = clf.module_\n\n# Adapt to target domain\nprint(\"\\n\" + \"=\" * 50)\nprint(\"Adapting SPDBatchNorm to Target Domain\")\nprint(\"=\" * 50)\nadapted_model = adapt_spdbn(underlying_model, X_target, n_passes=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluating After Adaptation\n\n<div class=\"alert alert-info\"><h4>Note</h4><p>Cross-session transfer typically shows distribution shifts\n   that SPDBatchNorm can correct.\n   The improvement depends on:\n\n   - Non-stationarity between sessions\n   - Training convergence on the source session\n   - How well the learned features generalize\n\n   Typical improvements range from 3-10% for cross-session transfer.</p></div>\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Convert target data for prediction\nX_target_tensor = torch.tensor(X_target, dtype=torch.float32)\nif next(adapted_model.parameters()).is_cuda:\n    X_target_tensor = X_target_tensor.cuda()\n\n# Predict with adapted model\nwith torch.no_grad():\n    logits = adapted_model(X_target_tensor)\n    y_pred_target_adapted = logits.argmax(dim=1).cpu().numpy()\n\ntarget_acc_adapted = accuracy_score(y_target, y_pred_target_adapted)\nimprovement = target_acc_adapted - target_acc_no_adapt\n\nprint(f\"\\n{'='*50}\")\nprint(\"Results WITH Domain Adaptation\")\nprint(f\"{'='*50}\")\nprint(f\"Target Accuracy (No Adaptation):   {target_acc_no_adapt*100:.2f}%\")\nprint(f\"Target Accuracy (With Adaptation): {target_acc_adapted*100:.2f}%\")\nif improvement >= 0:\n    print(f\"Improvement: +{improvement*100:.2f}%\")\nelse:\n    print(f\"Improvement: {improvement*100:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Visualizing Results\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n\n# Accuracy comparison\nax1 = axes[0]\nconditions = [\"Source\\n(Train)\", \"Target\\n(No Adapt)\", \"Target\\n(Adapted)\"]\naccuracies = [source_acc * 100, target_acc_no_adapt * 100, target_acc_adapted * 100]\ncolors = [\"#2ecc71\", \"#e74c3c\", \"#3498db\"]\nbars = ax1.bar(conditions, accuracies, color=colors, edgecolor=\"black\", linewidth=1.5)\nax1.set_ylabel(\"Accuracy (%)\", fontsize=12)\nax1.set_title(\"Domain Adaptation Results\", fontsize=14)\nax1.set_ylim([0, 100])\nax1.axhline(y=25, color=\"gray\", linestyle=\"--\", alpha=0.5, label=\"Chance level\")\n\n# Add value labels\nfor bar, acc in zip(bars, accuracies):\n    ax1.text(\n        bar.get_x() + bar.get_width() / 2,\n        bar.get_height() + 2,\n        f\"{acc:.1f}%\",\n        ha=\"center\",\n        va=\"bottom\",\n        fontsize=11,\n        fontweight=\"bold\",\n    )\n\n# Add improvement annotation with arrow between target bars\nif improvement != 0:\n    arrow_y = max(target_acc_no_adapt, target_acc_adapted) * 100 + 15\n    ax1.annotate(\n        \"\",\n        xy=(2, target_acc_adapted * 100 + 5),\n        xytext=(1, target_acc_no_adapt * 100 + 5),\n        arrowprops=dict(\n            arrowstyle=\"->\",\n            color=\"green\" if improvement > 0 else \"red\",\n            lw=2,\n            connectionstyle=\"arc3,rad=0.2\",\n        ),\n    )\n    sign = \"+\" if improvement > 0 else \"\"\n    ax1.text(\n        1.5,\n        arrow_y,\n        f\"{sign}{improvement*100:.1f}%\",\n        ha=\"center\",\n        fontsize=10,\n        fontweight=\"bold\",\n        color=\"green\" if improvement > 0 else \"red\",\n    )\n\nax1.legend()\n\n# Training history\nax2 = axes[1]\nhistory = clf.history\nepochs = range(1, len(history) + 1)\nax2.plot(epochs, history[:, \"train_loss\"], \"b-\", label=\"Train Loss\", linewidth=2)\nax2.plot(epochs, history[:, \"valid_loss\"], \"r--\", label=\"Valid Loss\", linewidth=2)\nax2.set_xlabel(\"Epoch\", fontsize=12)\nax2.set_ylabel(\"Loss\", fontsize=12)\nax2.set_title(\"Training History (Source Domain)\", fontsize=14)\nax2.legend(fontsize=10)\nax2.grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Understanding SPDBatchNorm Adaptation\n\nThe key insight is that **session variability manifests as a shift in\nthe distribution of SPD matrices**. SPDBatchNorm counters this by:\n\n1. **Centering**: Removes the batch mean (Fr\u00e9chet mean on SPD manifold)\n\n   .. math::\n\n      \\tilde{P}_i = G^{-1/2} P_i G^{-1/2}\n\n2. **Scaling**: Normalizes dispersion\n\n   .. math::\n\n      \\hat{P}_i = \\tilde{P}_i^{w/\\sqrt{\\sigma^2 + \\varepsilon}}\n\nWhen we adapt, we update the running mean $G$ and variance\n$\\sigma^2$ to match the target domain, aligning the distributions\nwithout any labeled data.\n\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n\nIn this tutorial, we demonstrated:\n\n1. Training TSMNet on source session\n2. Observing performance drop on target session\n3. Adapting SPDBatchNorm statistics using unlabeled target data\n4. Achieving improved cross-session transfer performance\n\nCross-session non-stationarity is a key challenge in BCI. SPDBatchNorm\nadaptation compensates for these shifts by re-centering the covariance\ndistribution.\n\nThis **source-free unsupervised domain adaptation** is particularly\nvaluable in BCI applications where:\n\n- Calibration time should be minimized\n- Users return for multiple sessions\n- Signal properties drift over time\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Cleanup\nimport matplotlib.pyplot as plt\nplt.close('all')"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}